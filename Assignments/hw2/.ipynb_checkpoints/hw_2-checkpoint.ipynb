{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science\n",
    "## Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: Roshan Kumar\n",
    "\n",
    "Student Netid: rk3110\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a Training Set and Training a Decision Tree\n",
    "This is a hands-on task where we build a predictive model using Decision Trees discussed in class. For this part, we will be using the data in `cell2cell_data.csv`.\n",
    "\n",
    "These historical data consist of 39,859 customers: 19,901 customers that churned (i.e., left the company) and 19,958 that did not churn (see the `\"churndep\"` variable). Here are the data set's 11 possible predictor variables for churning behavior: \n",
    "\n",
    "```\n",
    "Pos.  Var. Name  Var. Description\n",
    "----- ---------- --------------------------------------------------------------\n",
    "1     revenue    Mean monthly revenue in dollars\n",
    "2     outcalls   Mean number of outbound voice calls\n",
    "3     incalls    Mean number of inbound voice calls\n",
    "4     months     Months in Service\n",
    "5     eqpdays    Number of days the customer has had his/her current equipment\n",
    "6     webcap     Handset is web capable\n",
    "7     marryyes   Married (1=Yes; 0=No)\n",
    "8     travel     Has traveled to non-US country (1=Yes; 0=No)\n",
    "9     pcown      Owns a personal computer (1=Yes; 0=No)\n",
    "10    creditcd   Possesses a credit card (1=Yes; 0=No)\n",
    "11    retcalls   Number of calls previously made to retention team\n",
    "```\n",
    "\n",
    "The 12th column, the dependent variable `\"churndep\"`, equals 1 if the customer churned, and 0 otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Load the data and prepare it for modeling. Note that the features are already processed for you, so the only thing needed here is split the data into training and testing. Use pandas to create two data frames: train_df and test_df, where train_df has 80% of the data chosen uniformly at random without replacement (test_df should have the other 20%). Also, make sure to write your own code to do the splits. You may use any random() function numpy but DO NOT use the data splitting functions from Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39859\n",
      "31887\n",
      "7972\n",
      "            count        mean         std  min     25%     50%      75%  \\\n",
      "revenue   31887.0   58.559224   43.472609  0.0   33.25   48.37   71.075   \n",
      "outcalls  31887.0   24.850898   34.662587  0.0    3.00   13.00   33.000   \n",
      "incalls   31887.0    7.985066   16.342498  0.0    0.00    2.00    8.670   \n",
      "months    31887.0   18.786151    9.593969  6.0   11.00   16.00   24.000   \n",
      "eqpdays   31887.0  392.198984  255.033744 -5.0  213.00  342.00  530.000   \n",
      "webcap    31887.0    0.895914    0.305377  0.0    1.00    1.00    1.000   \n",
      "marryyes  31887.0    0.366482    0.481851  0.0    0.00    0.00    1.000   \n",
      "travel    31887.0    0.057327    0.232471  0.0    0.00    0.00    0.000   \n",
      "pcown     31887.0    0.186847    0.389795  0.0    0.00    0.00    0.000   \n",
      "creditcd  31887.0    0.675134    0.468332  0.0    0.00    1.00    1.000   \n",
      "retcalls  31887.0    0.044187    0.224338  0.0    0.00    0.00    0.000   \n",
      "churndep  31887.0    0.498291    0.500005  0.0    0.00    0.00    1.000   \n",
      "\n",
      "              max  \n",
      "revenue    847.82  \n",
      "outcalls   610.33  \n",
      "incalls    404.00  \n",
      "months      61.00  \n",
      "eqpdays   1823.00  \n",
      "webcap       1.00  \n",
      "marryyes     1.00  \n",
      "travel       1.00  \n",
      "pcown        1.00  \n",
      "creditcd     1.00  \n",
      "retcalls     4.00  \n",
      "churndep     1.00  \n",
      "           count        mean         std   min       25%     50%     75%  \\\n",
      "revenue   7972.0   58.932176   46.727551 -5.86   33.7575   48.36   70.28   \n",
      "outcalls  7972.0   24.967447   34.861418  0.00    3.3300   13.00   34.00   \n",
      "incalls   7972.0    8.118713   17.719265  0.00    0.0000    2.00    9.00   \n",
      "months    7972.0   18.802057    9.598329  6.00   11.0000   17.00   24.00   \n",
      "eqpdays   7972.0  390.882715  257.913388 -5.00  210.0000  339.00  531.00   \n",
      "webcap    7972.0    0.893628    0.308333  0.00    1.0000    1.00    1.00   \n",
      "marryyes  7972.0    0.360763    0.480252  0.00    0.0000    0.00    1.00   \n",
      "travel    7972.0    0.056197    0.230316  0.00    0.0000    0.00    0.00   \n",
      "pcown     7972.0    0.179503    0.383797  0.00    0.0000    0.00    0.00   \n",
      "creditcd  7972.0    0.679002    0.466889  0.00    0.0000    1.00    1.00   \n",
      "retcalls  7972.0    0.043026    0.223520  0.00    0.0000    0.00    0.00   \n",
      "churndep  7972.0    0.503261    0.500021  0.00    0.0000    1.00    1.00   \n",
      "\n",
      "              max  \n",
      "revenue    861.11  \n",
      "outcalls   437.00  \n",
      "incalls    519.33  \n",
      "months      60.00  \n",
      "eqpdays   1812.00  \n",
      "webcap       1.00  \n",
      "marryyes     1.00  \n",
      "travel       1.00  \n",
      "pcown        1.00  \n",
      "creditcd     1.00  \n",
      "retcalls     3.00  \n",
      "churndep     1.00  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Code here\n",
    "# 1.1 load data\n",
    "# Use double slash with U to indicate start of avoid the effect of \\U which is interpreted as a special Unicode character.\n",
    "data = pd.read_csv('C:\\\\Users\\kumar\\OneDrive\\Documents\\GitHub\\DS-GA-1001-IntroToDataScience\\Assignments\\hw2\\data\\cell2cell_data.csv')\n",
    "\n",
    "# 1.2 Split the data in train and test datasets\n",
    "def split_data(data,percent):\n",
    "    # get list of indices for training data randomly\\\n",
    "    # random.sample(given_list,k) randomly samples a list of length k from given_list\n",
    "    train_indices = random.sample(range(0,len(data)), int(percent*len(data)))\n",
    "    test_indices  = list(set(range(0,len(data))) - set(train_indices))\n",
    "    # Use .ix instead of .iloc or .loc for accessing though both works(http://pandas.pydata.org/pandas-docs/stable/indexing.html)\n",
    "    return data.ix[train_indices],data.ix[test_indices]\n",
    "\n",
    "# split the data\n",
    "train_df,test_df = split_data(data,0.8000)\n",
    "print(len(data))\n",
    "print(len(train_df))\n",
    "print(len(test_df))\n",
    "\n",
    "print(train_df.describe().transpose())\n",
    "print(test_df.describe().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. If we had to, how would we prove to ourselves or a colleague that our data was indeed randomly sampled on X? And by prove, I mean empirically, not just showing this person our code. Don't actually do the work, just describe in your own words a test you could here. Hint: think about this in terms of selection bias and use notes from our 2nd lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selection bias; no selection bias in training and test sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Now build and train a decision tree classifier using `DecisionTreeClassifier()` [(manual page)](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) on train_df to predict the `\"churndep\"` target variable. Make sure to use `criterion='entropy'` when instantiating an instance of `DecisionTreeClassifier()`. For all other settings you should use all of the default options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# Code here\n",
    "features = ['revenue', 'outcalls', 'incalls', 'months', 'eqpdays', 'webcap', 'marryyes', 'travel', 'pcown', 'creditcd', 'retcalls']\n",
    "target = ['churndep']\n",
    "X_train = train_df[features]\n",
    "y_train = train_df[target]\n",
    "\n",
    "# Specify tree specifications\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "# Build the tree\n",
    "clf = clf.fit(X_train,y_train) # need to figure out the X and Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Using the resulting model from 2.2, show a bar plot of feature names and their feature importance (hint: check the attributes of the `DecisionTreeClassifier()` object directly in IPython or check the manual!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DecisionTreeClassifier in module sklearn.tree.tree object:\n",
      "\n",
      "class DecisionTreeClassifier(BaseDecisionTree, sklearn.base.ClassifierMixin)\n",
      " |  A decision tree classifier.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <tree>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  criterion : string, optional (default=\"gini\")\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |  \n",
      " |  splitter : string, optional (default=\"best\")\n",
      " |      The strategy used to choose the split at each node. Supported\n",
      " |      strategies are \"best\" to choose the best split and \"random\" to choose\n",
      " |      the best random split.\n",
      " |  \n",
      " |  max_features : int, float, string or None, optional (default=None)\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |          - If int, then consider `max_features` features at each split.\n",
      " |          - If float, then `max_features` is a percentage and\n",
      " |            `int(max_features * n_features)` features are considered at each\n",
      " |            split.\n",
      " |          - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |          - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |          - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |          - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_depth : int or None, optional (default=None)\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int, float, optional (default=2)\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a percentage and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for percentages.\n",
      " |  \n",
      " |  min_samples_leaf : int, float, optional (default=1)\n",
      " |      The minimum number of samples required to be at a leaf node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a percentage and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for percentages.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_leaf_nodes : int or None, optional (default=None)\n",
      " |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  class_weight : dict, list of dicts, \"balanced\" or None, optional (default=None)\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  min_impurity_split : float, optional (default=1e-7)\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |  \n",
      " |  presort : bool, optional (default=False)\n",
      " |      Whether to presort the data to speed up the finding of best splits in\n",
      " |      fitting. For the default settings of a decision tree on large\n",
      " |      datasets, setting this to true may slow down the training process.\n",
      " |      When using either a smaller dataset or a restricted depth, this may\n",
      " |      speed up the training.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : array of shape = [n_classes] or a list of such arrays\n",
      " |      The classes labels (single output problem),\n",
      " |      or a list of arrays of class labels (multi-output problem).\n",
      " |  \n",
      " |  feature_importances_ : array of shape = [n_features]\n",
      " |      The feature importances. The higher, the more important the\n",
      " |      feature. The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance [4]_.\n",
      " |  \n",
      " |  max_features_ : int,\n",
      " |      The inferred value of max_features.\n",
      " |  \n",
      " |  n_classes_ : int or list\n",
      " |      The number of classes (for single output problems),\n",
      " |      or a list containing the number of classes for each\n",
      " |      output (for multi-output problems).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  tree_ : Tree object\n",
      " |      The underlying Tree object.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  DecisionTreeRegressor\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      " |  \n",
      " |  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
      " |         and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
      " |  \n",
      " |  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
      " |         Learning\", Springer, 2009.\n",
      " |  \n",
      " |  .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
      " |         http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.model_selection import cross_val_score\n",
      " |  >>> from sklearn.tree import DecisionTreeClassifier\n",
      " |  >>> clf = DecisionTreeClassifier(random_state=0)\n",
      " |  >>> iris = load_iris()\n",
      " |  >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
      " |  ...                             # doctest: +SKIP\n",
      " |  ...\n",
      " |  array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n",
      " |          0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DecisionTreeClassifier\n",
      " |      BaseDecisionTree\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.feature_selection.from_model._LearntSelectorMixin\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_split=1e-07, class_weight=None, presort=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted=None)\n",
      " |      Build a decision tree classifier from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The target values (class labels) as integers or strings.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples] or None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. Splits are also\n",
      " |          ignored if they would result in any single class carrying a\n",
      " |          negative weight in either child node.\n",
      " |      \n",
      " |      check_input : boolean, (default=True)\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      X_idx_sorted : array-like, shape = [n_samples, n_features], optional\n",
      " |          The indexes of the sorted training input samples. If many tree\n",
      " |          are grown on the same dataset, this allows the ordering to be\n",
      " |          cached between trees. If None, the data will be sorted here.\n",
      " |          Don't use this parameter unless you know what to do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities of the input samples X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class log-probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute `classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X, check_input=True)\n",
      " |      Predict class probabilities of the input samples X.\n",
      " |      \n",
      " |      The predicted class probability is the fraction of samples of the same\n",
      " |      class in a leaf.\n",
      " |      \n",
      " |      check_input : boolean, (default=True)\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute `classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  apply(self, X, check_input=True)\n",
      " |      Returns the index of the leaf that each sample is predicted as.\n",
      " |      \n",
      " |      .. versionadded:: 0.17\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : boolean, (default=True)\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape = [n_samples,]\n",
      " |          For each datapoint x in X, return the index of the leaf x\n",
      " |          ends up in. Leaves are numbered within\n",
      " |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
      " |          numbering.\n",
      " |  \n",
      " |  decision_path(self, X, check_input=True)\n",
      " |      Return the decision path in the tree\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : boolean, (default=True)\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      " |          Return a node indicator matrix where non zero elements\n",
      " |          indicates that the samples goes through the nodes.\n",
      " |  \n",
      " |  predict(self, X, check_input=True)\n",
      " |      Predict class or regression value for X.\n",
      " |      \n",
      " |      For a classification model, the predicted class for each sample in X is\n",
      " |      returned. For a regression model, the predicted value based on X is\n",
      " |      returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : boolean, (default=True)\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The predicted classes, or the predict values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances.\n",
      " |      \n",
      " |      The importance of a feature is computed as the (normalized) total\n",
      " |      reduction of the criterion brought by that feature.\n",
      " |      It is also known as the Gini importance.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array, shape = [n_features]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.feature_selection.from_model._LearntSelectorMixin:\n",
      " |  \n",
      " |  transform(*args, **kwargs)\n",
      " |      DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      " |      \n",
      " |      Reduce X to its most important features.\n",
      " |      \n",
      " |              Uses ``coef_`` or ``feature_importances_`` to determine the most\n",
      " |              important features.  For models with a ``coef_`` for each class, the\n",
      " |              absolute sum over the classes is used.\n",
      " |      \n",
      " |              Parameters\n",
      " |              ----------\n",
      " |              X : array or scipy sparse matrix of shape [n_samples, n_features]\n",
      " |                  The input samples.\n",
      " |      \n",
      " |              threshold : string, float or None, optional (default=None)\n",
      " |                  The threshold value to use for feature selection. Features whose\n",
      " |                  importance is greater or equal are kept while the others are\n",
      " |                  discarded. If \"median\" (resp. \"mean\"), then the threshold value is\n",
      " |                  the median (resp. the mean) of the feature importances. A scaling\n",
      " |                  factor (e.g., \"1.25*mean\") may also be used. If None and if\n",
      " |                  available, the object attribute ``threshold`` is used. Otherwise,\n",
      " |                  \"mean\" is used by default.\n",
      " |      \n",
      " |              Returns\n",
      " |              -------\n",
      " |              X_r : array of shape [n_samples, n_selected_features]\n",
      " |                  The input samples with only the selected features.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numpy array of shape [n_samples, n_features]\n",
      " |          Training set.\n",
      " |      \n",
      " |      y : numpy array of shape [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEtCAYAAAAbeVcBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xu8ZXP9x/HXe4YZZsbduM1gXMZlEpVxKUqIXNJQRIki\nBrn8/H6pVIqQSCSl5JZLhMplyiAkiWSG5BalwY/p4hKR/OTy+f3x+W5nzXZmzjrn7H3Onpn38/E4\nj7PX2mut73ftvdb6fG9rbUUEZmZmPRky2BkwM7O5gwOGmZnV4oBhZma1OGCYmVktDhhmZlaLA4aZ\nmdXigGFmZrU4YNigkfSIpBcl/avyt0I/t/luSY+3Ko810zxX0rEDmebsSDpK0g8GOx82b3LAsMG2\nQ0SMqvz9ZTAzI2mBwUy/P+bmvNvcwQHDOpKkjSXdKulZSb+X9O7Ke3tJ+oOk5yXNkLRfmT8SuBpY\noVpjaa4BNNdCSk3ns5LuBl6QtEBZ7yeSnpT0sKRDauZ7nKQoeXxM0jOS9pe0gaS7y/58u7L8xyXd\nIunbkv4p6QFJW1beX0HSFEn/kPSQpH0r7x0l6ceSfiDpOWB/4PPArmXffz+nz6v6WUj6lKQnJP1V\n0l6V9xeWdJKkR0v+fi1p4Z6+I5s3uURiHUfSGOAqYA/gGmBL4CeS1oqIJ4EngPcBM4B3AVdLmhYR\nd0raFvhBRIytbK9Osh8GtgeeAl4DfgpcWeaPBa6X9GBEXFtzNzYCxpf8TSn78R5gQeB3kn4UETdV\nlv0xsDTwAeAySatExD+Ai4F7gRWAtYDrJP05In5R1p0E7ALsCQwv21g9Ij5ayctsP6/y/nLAYsAY\nYCvgx5KuiIhngK8DbwLeAfyt5PW1Gt+RzYNcw7DBdkUpoT4r6Yoy76PA1IiYGhGvRcR1wHRgO4CI\nuCoi/hzpJuDnwDv7mY9TI+KxiHgR2AAYHRFHR8R/ImIGcCawWy+2d0xE/F9E/Bx4AfhhRDwRETOB\nm4G3VpZ9AjglIl6OiEuAB4HtJa0IbAJ8tmzrLuAsMjg0/CYiriif04vdZaTG5/UycHRJfyrwL2BN\nSUOAvYH/ioiZEfFqRNwaES/Rw3dk8ybXMGyw7RgR1zfNWxnYRdIOlXkLAjcClFrEkcAaZKFnBHBP\nP/PxWFP6K0h6tjJvKHmhr+vvldcvdjM9qjI9M2Z9CuijZI1iBeAfEfF803sTZ5PvbtX4vJ6OiFcq\n0/8u+VsaWAj4czebneN3ZPMmBwzrRI8BF0TEvs1vSBoO/IQsZV8ZES+Xmkmj3am7xy+/QF4kG5br\nZpnqeo8BD0fE+L5kvg/GSFIlaKxENmP9BVhS0iKVoLESMLOybvP+zjJd4/Oak6eA/wNWA37f9N5s\nvyObd7lJyjrRD4AdJL1X0lBJC5XO2bHAMLKt/knglVJ63rqy7t+BpSQtVpl3F7CdpCUlLQcc2kP6\ntwPPl47whUse1pG0Qcv2cFbLAIdIWlDSLsDaZHPPY8CtwFfLZ7Au8Any85mdvwPjSnMS9Px5zVZE\nvAacA5xcOt+HSnp7CUJz+o5sHuWAYR2nXCgnkSN+niRLs58GhpSS9iHApcAzwEfI0nhj3QeAHwIz\nSr/ICsAFZAn5EbL9/pIe0n+V7CR+C/AwWdI+i+wYboffkh3kTwFfAXaOiKfLex8GxpG1jcuBI7tp\nwqv6Ufn/tKQ7e/q8ajiMbL6aBvwDOIH8Hmb7HfVi2zaXkX9AyWzwSPo4sE9EbDrYeTHriUsDZmZW\niwOGmZnV4iYpMzOrxTUMMzOrxQHDzMxqmadu3Ft66aVj3Lhxg50NM7O5xh133PFURIyus+w8FTDG\njRvH9OnTBzsbZmZzDUmP1l3WTVJmZlaLA4aZmdXigGFmZrU4YJiZWS0OGGZmVosDhpmZ1eKAYWZm\ntThgmJlZLfPUjXv9Me7wq9qy3UeO374t2zUzG2iuYZiZWS0OGGZmVosDhpmZ1eKAYWZmtbQ1YEja\nRtKDkh6SdHg37+8u6W5J90i6VdJ6lfceKfPvkuRH0JqZDbK2jZKSNBQ4DdgKeByYJmlKRNxfWexh\nYLOIeEbStsAZwEaV9zePiKfalUczM6uvnTWMDYGHImJGRPwHuBiYVF0gIm6NiGfK5G3A2Dbmx8zM\n+qGdAWMM8Fhl+vEyb3Y+AVxdmQ7gekl3SJrchvyZmVkvdMSNe5I2JwPGppXZm0bETEnLANdJeiAi\nftXNupOByQArrbTSgOTXzGx+1M4axkxgxcr02DJvFpLWBc4CJkXE0435ETGz/H8CuJxs4nqDiDgj\nIiZGxMTRo2v9LK2ZmfVBOwPGNGC8pFUkDQN2A6ZUF5C0EnAZsEdE/LEyf6SkRRqvga2Be9uYVzMz\n60HbmqQi4hVJBwHXAkOBcyLiPkn7l/dPB74ELAV8RxLAKxExEVgWuLzMWwC4KCKuaVdebd7g54GZ\ntVdb+zAiYiowtWne6ZXX+wD7dLPeDGC95vlmZjZ4fKe3mZnV4oBhZma1OGCYmVktDhhmZlaLA4aZ\nmdXigGFmZrU4YJiZWS0OGGZmVosDhpmZ1eKAYWZmtThgmJlZLQ4YZmZWiwOGmZnV4oBhZma1OGCY\nmVktDhhmZlaLA4aZmdXigGFmZrU4YJiZWS0OGGZmVosDhpmZ1eKAYWZmtThgmJlZLQ4YZmZWiwOG\nmZnV4oBhZma1OGCYmVktDhhmZlaLA4aZmdXigGFmZrU4YJiZWS1tDRiStpH0oKSHJB3ezfu7S7pb\n0j2SbpW0Xt11zcxsYLUtYEgaCpwGbAtMAD4saULTYg8Dm0XEm4FjgDN6sa6ZmQ2gdtYwNgQeiogZ\nEfEf4GJgUnWBiLg1Ip4pk7cBY+uua2ZmA6udAWMM8Fhl+vEyb3Y+AVzd23UlTZY0XdL0J598sh/Z\nNTOzOemITm9Jm5MB47O9XTcizoiIiRExcfTo0a3PnJmZAbBAG7c9E1ixMj22zJuFpHWBs4BtI+Lp\n3qxrZmYDp8cahqQ1JN0g6d4yva6kI2psexowXtIqkoYBuwFTmra9EnAZsEdE/LE365qZ2cCq0yR1\nJvA54GWAiLibvIDPUUS8AhwEXAv8Abg0Iu6TtL+k/ctiXwKWAr4j6S5J0+e0bq/2zMzMWqpOk9SI\niLhdUnXeK3U2HhFTgalN806vvN4H2KfuumZmNnjq1DCekrQaEACSdgb+2tZcmZlZx6lTwziQvKFu\nLUkzyZvtPtrWXJmZWcfpMWBExAzgPZJGAkMi4vn2Z8vMzDpNnVFSx0laPCJeiIjnJS0h6diByJyZ\nmXWOOk1S20bE5xsTEfGMpO2AOkNrbTbGHX5V27b9yPHbt23bZjb/qtPpPVTS8MaEpIWB4XNY3szM\n5kF1ahgXAjdI+n6Z3gs4r31ZMjOzTlSn0/sESXcDW5ZZx0TEte3NlpmZdZpaz5KKiKvpepKsmZnN\nh+qMkvqApD9J+qek5yQ9L+m5gcicmZl1jjo1jK8BO0TEH9qdGTMz61x1Rkn93cHCzMzq1DCmS7oE\nuAJ4qTEzIi5rW67MzKzj1AkYiwL/BrauzAvydyzMzGw+UWdY7V4DkREzM+tsPQYMSQuRv7f9JmCh\nxvyI2LuN+TIzsw5Tp9P7AmA54L3ATeTva/uJtWZm85k6AWP1iPgi8EJEnAdsD2zU3myZmVmnqRMw\nXi7/n5W0DrAYsEz7smRmZp2oziipMyQtQT7OfAowCvhiW3NlZmYdp07AuCEingF+BawKIGmVtubK\nzMw6Tp0mqZ90M+/Hrc6ImZl1ttnWMCStRQ6lXUzSBypvLUpleK2Zmc0f5tQktSbwPmBxYIfK/OeB\nfduZKTMz6zyzDRgRcaWknwGfjYjjBjBPZmbWgebYhxERrwI7DlBezMysg9UZJXWLpG8DlwAvNGZG\nxJ1ty5WZmXWcOgHjLeX/0ZV5AWzR+uyYmVmnqvO02s0HIiNmZtbZ6vym92KSTpY0vfydJGmxgcic\nmZl1jjo37p1DDqX9UPl7Dvh+nY1L2kbSg5IeknR4N++vJek3kl6SdFjTe49IukfSXZKm10nPzMza\np04fxmoR8cHK9Jcl3dXTSpKGAqcBWwGPA9MkTYmI+yuL/QM4hNmPxNo8Ip6qkUczM2uzOjWMFyVt\n2piQtAnwYo31NgQeiogZEfEf4GJgUnWBiHgiIqbR9URcMzPrUHVqGAcA55V+C5G1go/VWG8M8Fhl\n+nF69zsaAVwv6VXgexFxRi/WNTOzFqszSuouYD1Ji5bp59qeq7RpRMyUtAxwnaQHIuJXzQtJmgxM\nBlhppZUGKGtmZvOfOqOklpJ0KvBL4EZJ35S0VI1tzwRWrEyPLfNqiYiZ5f8TwOVkE1d3y50RERMj\nYuLo0aPrbt7MzHqpTh/GxcCTwAeBncvrS2qsNw0YL2kVScOA3cgfYOqRpJGSFmm8BrYG7q2zrpmZ\ntUedPozlI+KYyvSxknbtaaWIeEXSQcC1wFDgnIi4T9L+5f3TJS0HTCcfmf6apEOBCcDSwOWSGnm8\nKCKu6c2OmZlZa9UJGD+XtBtwaZnemQwCPYqIqcDUpnmnV17/jWyqavYcsF6dNMzMbGDUaZLaF7gI\n+E/5uxjYT9LzkgaqA9zMzAZZnVFSiwxERszMrLPVaZJC0rrAuOryEXFZm/JkZmYdqMeAIekcYF3g\nPuC1MjsABwwzs/lInRrGxhExoe05MTOzjlan0/s3khwwzMzmc3VqGOeTQeNvwEvk86QiItZta87M\nzKyj1AkYZwN7APfQ1Ydhc5lxh1/Vtm0/cvz2bdu2mXWOOgHjyYio9UgPMzObd9UJGL+TdBHwU7JJ\nCvCwWjOz+U2dgLEwGSi2rszzsFozs/lMnTu99xqIjJiZWWebbcCQ9C2yJtGtiDikLTkyM7OONKca\nxvQBy4WZmXW82QaMiDhvIDNiZmadrc6d3mZmZg4YZmZWjwOGmZnV0mPAkLSGpBsk3Vum15V0RPuz\nZmZmnaRODeNM4HPAywARcTewWzszZWZmnadOwBgREbc3zXulHZkxM7POVSdgPCVpNcpNfJJ2Bv7a\n1lyZmVnHqfMsqQOBM4C1JM0EHgZ2b2uuzMys48wxYEgaAkyMiPdIGgkMiYjnByZrZmbWSebYJBUR\nrwGfKa9fcLAwM5t/1enDuF7SYZJWlLRk46/tOTMzs45Spw9j1/L/wMq8AFZtfXbMzKxT1fk9jFUG\nIiNmZtbZegwYkvbsbn5EnN/67JiZWaeq0yS1QeX1QsCWwJ2AA4aZ2XykTpPUwdVpSYsDF7ctR2Zm\n1pH68rTaF4Ba/RqStpH0oKSHJB3ezftrSfqNpJckHdabdc3MbGDV6cP4KV2/7T0EmAD8qMZ6Q4HT\ngK2Ax4FpkqZExP2Vxf4BHALs2Id1zcxsANXpw/h65fUrwKMR8XiN9TYEHoqIGQCSLgYmAa9f9CPi\nCeAJSdv3dl0zMxtYdZqktouIm8rfLRHxuKQTaqw3BnisMv14mVdHf9Y1M7M2qBMwtupm3ratzkhf\nSZosabqk6U8++eRgZ8fMbJ4124Ah6QBJ9wBrSrq78vcwcHeNbc8EVqxMjy3z6qi9bkScERETI2Li\n6NGja27ezMx6a059GBcBVwNfBaqjlJ6PiH/U2PY0YLykVciL/W7AR2rmqz/rmplZG8w2YETEP4F/\nAh8GkLQMeePeKEmjIuJ/57ThiHhF0kHAtcBQ4JyIuE/S/uX90yUtB0wHFgVek3QoMCEinutu3f7u\nrJmZ9V2dYbU7ACcDKwBPACsDfwDe1NO6ETEVmNo07/TK67+RzU211jUzs8FTp9P7WGBj4I/lQYRb\nAre1NVdmZtZx6gSMlyPiaWCIpCERcSMwsc35MjOzDlPnxr1nJY0CbgYulPQE+XgQMzObj9SpYUwC\n/g0cClwD/BnYoZ2ZMjOzzlPnabUvSFoZGB8R50kaQY5cMjOz+UiPNQxJ+wI/Br5XZo0BrmhnpszM\nrPPU6cM4kHwY4G8BIuJP5Z4Ms9kad/hVbdv2I8c3P6vSzAZCnT6MlyLiP40JSQvQ9bhzMzObT9QJ\nGDdJ+jywsKStyN/C+Gl7s2VmZp2mTsA4HHgSuAfYj7z7+oh2ZsrMzDrPbPswJK0UEf8bEa8BZ5Y/\nMzObT82phvH6SChJPxmAvJiZWQebU8BQ5fWq7c6ImZl1tjkFjJjNazMzmw/N6T6M9SQ9R9Y0Fi6v\nKdMREYu2PXdmZtYx5vQDSn78h5mZva7OsFozMzMHDDMzq8cBw8zManHAMDOzWhwwzMysFgcMMzOr\nxQHDzMxqccAwM7NaHDDMzKwWBwwzM6vFAcPMzGpxwDAzs1ocMMzMrBYHDDMzq8UBw8zMamlrwJC0\njaQHJT0k6fBu3pekU8v7d0t6W+W9RyTdI+kuSdPbmU8zM+vZnH5xr18kDQVOA7YCHgemSZoSEfdX\nFtsWGF/+NgK+W/43bB4RT7Urj2ZmVl87axgbAg9FxIyI+A9wMTCpaZlJwPmRbgMWl7R8G/NkZmZ9\n1M6AMQZ4rDL9eJlXd5kArpd0h6TJbculmZnV0rYmqRbYNCJmSloGuE7SAxHxq+aFSjCZDLDSSisN\ndB7NzOYb7axhzARWrEyPLfNqLRMRjf9PAJeTTVxvEBFnRMTEiJg4evToFmXdzMyatTNgTAPGS1pF\n0jBgN2BK0zJTgD3LaKmNgX9GxF8ljZS0CICkkcDWwL1tzKuZmfWgbU1SEfGKpIOAa4GhwDkRcZ+k\n/cv7pwNTge2Ah4B/A3uV1ZcFLpfUyONFEXFNu/JqZmY9a2sfRkRMJYNCdd7pldcBHNjNejOA9dqZ\nNzMz6x3f6W1mZrU4YJiZWS0OGGZmVosDhpmZ1eKAYWZmtThgmJlZLQ4YZmZWiwOGmZnV4oBhZma1\nOGCYmVktDhhmZlaLA4aZmdXigGFmZrU4YJiZWS2d/BOtZlYx7vCr2rLdR47fvi3btXmPaxhmZlaL\nA4aZmdXiJikz6wjtanIDN7u1imsYZmZWiwOGmZnV4oBhZma1OGCYmVktDhhmZlaLA4aZmdXiYbVm\nNl/yMN7ec8Aws275gmrN3CRlZma1OGCYmVktDhhmZlaLA4aZmdXiTm+zPvLvU1hvze3HTFtrGJK2\nkfSgpIckHd7N+5J0ann/bklvq7uumZkNrLYFDElDgdOAbYEJwIclTWhabFtgfPmbDHy3F+uamdkA\namcNY0PgoYiYERH/AS4GJjUtMwk4P9JtwOKSlq+5rpmZDaB2BowxwGOV6cfLvDrL1FnXzMwG0Fzf\n6S1pMtmcBfAvSQ8OQLJLA0/VWVAnDGx6LUpzoNPrVZpOb+5ObzDSdHpztHLdBdsZMGYCK1amx5Z5\ndZZZsMa6AETEGcAZ/c1sb0iaHhETnd7cm6bTm7vTG4w05/X06mhnk9Q0YLykVSQNA3YDpjQtMwXY\ns4yW2hj4Z0T8tea6ZmY2gNpWw4iIVyQdBFwLDAXOiYj7JO1f3j8dmApsBzwE/BvYa07rtiuvZmbW\ns7b2YUTEVDIoVOedXnkdwIF11+0gA9oENh+kNxhpOr25O73BSHNeT69Hymu2mZnZnPlZUmZmbSJJ\ng52HVnLAsLnSYJyIA5XmvHaRAZC01GDnYZAsBvPOd+qAMcDafeCUx6q0XRnZNhgX7aHwev/X63kZ\nqDQltbXfT9KQgdy3AfRTSYfBPLVPcyTpYODHMOvx2oZ0hjZNt+3zdcAYQOWxJ6PbmUZEvCppQUnv\nb/W2GweipKHlcS5Rhj0P2EUgIl4t6e0p6YuNvLQjrcY+VdL8H+BUSau2Ia1GUHpN0uqSDpY0Fhi0\ni6ukdSQt0o/1VbmYfRH4mKQl2nnx7CYPG0jaZqDSK2k29vlsYISk95b5Lb/eluO/cXxuA+0NTg4Y\nA0DSUEmnApcBX5D0UUnDW7TtLSVtVZneB7gN2FHSiFakUbEAzHIBPQY4t7lU3ErNJ5mk5SVdA3wA\nuK4EyLZcVBv7JGkRSZcDbwO+FREz2pDWq5IWkLQjcC6wGXAi8OmShwE7VyVNkDQVOAFYso/baBQq\nXgWIiBuA+4EjW5fTOaa/pKSjyZFGx0o6VFJbC2sN5bscBuwKPAL8T5n/Wiu2L2nVRoGwpLWppBuB\nT0tq6yOUHDAGxoeApyPi7cAi5P0mq/Rng5JWKC/fChwsaYikxYCtgD0jYu+I+Hd/0qikNVTSicCX\nyvRoSbcASwBHtOpEaEpzUeg6ySpBYR3gxYjYEfhj+Rza0k4saVFJhwCvkvcDfQX4t6QNJW3YnzTL\n99W87rfJYLF/ROwMfB3YQ9Ka7fiMZ5OvYcBJwJURsX1EPNqX7ZQL2VKSjpa0t6SRwMHAJElvbmWe\nm0laEfgOMCEi3gocQB4372pTes2FmqWB35IPUZ0GLFeOo34F/kpt7z3AbpWa7mTg2IjYMiJmuklq\nLiTpLZLWL5MrkQfNeeRDFA+NiAea2x57se01gFslLQN8H3iGDEIjgEWByeVEvUzSYepjh2MpzR9Z\nSok3AutJGl/SmA6cDAyTtEVpPmmJ0nR3vqRNy/TngCPKRec2YB1JvwC+ClwCXAFtqYqvQdYqliJL\nxz8DPgUcDpwnab2+pClJEfFaadJbW9Jq5a0jgYXIz5eIuIO8F2nX/u9Kj3l6a7nQLQzcA6wtabKk\n4yR9srclV0nvJI+Zp8mfKDgd+CcZEI9paea70lxe+ds5Y4G7ye+PiJgGPAisL2ntFqc5pFKoWUPS\nqEb6EXFARHwDOAT4hKTFS5Njry/o5Zy4UNJmwE+A/wU+UK4h6wIbSTpG0inAKWpTX5sDRotVDob3\nASeXpqcngHcDt0TEeyPiHknrMevzsmqLiD+SF68vRcTTwJXATsDzZDPCo8BNZf47gGX7uDtBlnDf\nU26kvB/Yh3yu11LABcC+ZEn4pBLI+qwEBMgA+Euy5nQ+sCowkSzhLwBsSZYaP09+ruprwOqmdLiZ\npC3L5BBgXEQ8FhGHAxtFxCER8QHyKQS9alZsHBslUCxX9u1s8jj5UET8HfgW8LXKaq8Af+rLvtXM\n06aS7gT+G/ge+SC6a4CRJe2XgfWAzWtub73ycnUysF4EbAE8C7waEV8GVpW0Swv3YY2S7vPAO8kg\ndRZwn6S9y2I/BlYAtiy1qP6mOQRe73NaRtLFZAFqNPmZbSlpobL43eT+f64fST4P3ADsVc75m8la\n01iy5jYM+AMZpN9Ffg6tFxH+6+cfXTdAbgN8uDL/V2Rz1EbAN4CjyvwvkwfRW2tuf0gjjcq8ccBd\nwAbl/W8BRzctszlwC1k1r7svQ5um9wZ+WV6vB1wFbFimlyz/RwGXA2P68RmuCny+vF4J2ISsPf24\nzBtDNjPsDYws87Yg+4XOBob3Ic0hldcLlv/7k6XRN5XpK4AdGsuQj7K5CrgaWKy36VTmfQ04qLz+\nYTkeJpTpJ8q+H0gG/y3bcMwuUI6b04CJZd7TwBHAsMpyY8hazsazO+4r02sA5wFrk6Xqv5AFl+0b\naZb/+wCXtmg/hpXPbnrJ6+eAM8t7HyrH5dJl+iNk0O9PehOAZavnSjkuv9S03PnAd8vrkWTh6g/A\nMjXTUTfn4vhyrOxWvr8jyVq2KsssTQb8rVp9zESEA0ZLP8w82U6qXGzeTzahLFNOpkvLyXchsEIf\ntj8R2LRx0JEluKvK63cAPyWbUJYpJ8qvgE16sf3qBbRxMgwlS9OfLNOHAheV1yPKwXsz2bk4qvki\nUiPN6sH+C7LNd0rZ1z3I5pHGCb8XWYrbBFgTuB34RC/TG940PQY4h6wlrVPm7VfSOQU4DHhnZdnT\ngd37eHzsWU7y4eVvNfKCeiZZEv9WWW4n4EVg97oXmF7mYxLZ7r0cWZA5u3zun6ssM5bss3oA+NQc\nvrMFgdXL6xWB68ga7bZkc+FG5b0lyue5fov2YSfg4+X1LsDvyIvyW8ia9Wiyw/484JgWpLdqJd0L\ny/4dU/brGkpABRYq/5clA8RpZAHkEGDxPqS7JFmjHlGmdy3nxyJkYfF7wKTy3rnlc5jc6mPm9fy0\na8Pzwx9Z4r6BDBTrkBfqU8oFp1FivR/4Znm9AKVUXmPbQyuvR5FV7OlkkLitzB8O/BrYrUx/Ebik\nvK4dKJrSnUCWnr8K7FPmvZeszSwCLA/8gLz4LUWWhLfpQzpDmqaXIAPcX+gKiGPK53lUmV6EvLgf\nRra1v6Hk3kOabyFrJOPK9E7ArWTJ81CyGewt5b1GSe1fwH83fyfdTVfmN5e8R5UL1/VkU0Hj2JhM\nV5DYDPg/YLsyfTPlAk4fak9z+szJ5qfLyusry/e5XOWYej9ZGNiherx2850NK9u6ofKZXkD284wg\nL5L3kxfWu8hO/X7tSyWdDcmS/WfJZsldyaB/ajmOGsF/HUqNoB9pTgROKa/XJ5uX7iFr+UuUNLeh\n1KAq6y1Onjub9vGcOLx8bicBPyKb+RYmCy2fLcscVD7zYeSAlwVbcazMNo/t3Pi8+EdWFUeWk+Gk\ncsJ8iqw5jAR2JpuHNivL/4AsOfen+eIdwP+U1x8DXgOOK9O7kCXtoeWA+lgv9qX5Avg+soTyfrL0\n8hywRnnvXODE8novsrbU7QWzzmdYef1+4KPAImX628A3yuvh5IV0Cl3NJusDS/UyvWrN6XJyFBJk\n08nKwMbkBfpOsgYwpry/OnAUGRSHdpf/pnQWmM38FcpJPbKan7LfV5O1z6+U/dygvLcxMINK81A/\njtk1ySC7Y5lenmzTH062f38P+ERJ80byJ5EXqh4nTd/ZVmSQbQT2o8rn+rZyPB5GV/PTpmQ/15v7\nuQ/jyHPsl2SAWhNYixx6/EuyT2txshDwGvDB2R1zdY8Z4DPAuyvTm5Zj5hSyP7Kx7BFkjXTn8lmd\nSgayWgUa8prSHCzWBo4jrylbk8NzG4XBzcg+zLXKMbp/49hq91/bE5gX/8qX9BJdJd8lySDxpTJ9\nMFny/xNZiuz1hZW8kN5ENlsMJ4eOXkqWVN9FlkbHlmWvA3btx/40qrTLkSN0ti/5vxOYXt5bkyz9\nr1NOin5dyMgmgxOB35QL1O1l/tpk0FqrTI8jL+JH9ye9yrbOK/vWaDZci2xyexNZA7mverEhS9nH\nlNezvQClquzqAAARKUlEQVSU9X9dmf6v8jkuRwbfW8v8henq83oTcDTwR+BYupoBh/SUXs39FbAj\nWQqdTNYmPl6+w3PL+8PKsfYdMnjt3rSNarBdimw6u4ks/d4OjC7v/Xc5B35NDvFs9Tl3GnBkeX1U\nyceyZJC4newU3rq8/4b+ll6m1Qh2XyFr9ouQ/Qf/B4wv711J1/k+imw+/BlwB1mQ7Lbw0EO6a5Mj\noLYo383CZO3sVrKm8nu6+oNOAb7Q6s+5xzwOdIJz6185qc4hS1cib6j6WXlvCFltvYHSkU2W1lau\nsd2FKe2TlXmfLyfe+yvzVgcuqEw/BlxfXi/ax33ahKz9XEdXk8QaZCmz0QH7IiUYkbWbFfuQTnNN\nZrVyIl5XmXczsG95fSTZrHAM2TS2fB/SbG4WOphsRtiz/D+brlLjfWWZxvDZz1UuhJdTSnY9pUP2\nVx1VLhjXAd8s80QGog9Vlt2Krrb/pSvz+xUkmj9zsuR7fHn99nJBOoQc6fbWyvILNq2/QOX1MLL2\nuRRwQJm3Xzk2vkJpZiID45NkaXhUC/bhIDLoigxoO1beu5IcMQR5X8K1lD6N2R0DfUj/PeW4bDT5\nfg24uLx+J9m/szBZ612lvF66xnaH8Ma+tB3JPo//AhYu85Yg+7dWLNNTgYfL6173h7TkuBqMROem\nv3KwHk+WHnYmRz80hnc+SFepZgRZhfxaL7f9cbKKuRjwvjL/EmClxsFV/m9Flqp2KekcA+zXi7Te\nUMshq87NJcr3kh3YQ8j+jNsogakFn+W2dF2I9y4n+fqV935D1wiUE8pfr6razOaCS5bIPlZer1Yu\nBFuWz/1nZHPQXWRgqZaqP8Zs2oWbP1OykPA4pc+q8l0eR9ZeppVj5/Lyenw13/29wJXtbAF8kq7g\n+9ZyoVmsTK9FNrU8SbkQNo7F8n8spfZVpjcn+30uKMfrkuRIndPIvoO7S5pDK5/BEv3ch1XJWsxV\n5XMbQhYwdqer6fKTwA8q6/Sp0DSb9Jcu39GPSh4eImsyw8hzfquy3ElkAeNGSo245vbfXzkWGzWG\nUyn9V5Xl3lS2/Z5yXH6VrNn1uvbSss9msBLu9L9ycGxWXn+znCz/U06QXcr8fSkd0GW69kFbOUG3\nI0tk99PVdv8bYI9GPir52a2c/GdRaWPuYfvVtvehdDX1LA48DKxSphvt642RF9cA99KHIZ3ls6qW\nvBuDA24kR3IdSZaeTibbnhtNAFOA7zTnu7efaXn9oXKC7VSmv0MG28Z+foOsxSxGDlb4JGUkTHl/\ntp2HTemMIJt7xpXpE4FrK+8vVy4yi5AX609QSsYtPl7XJPvLriELF78ng//mZHPXkk35vpSuEnp1\n/oSyjc3I2sijzFqzHQvcUJm+jyxE9boW2M0+jCr/d6GpuaWcJ5eQF85VyeHOO8/puKuZZncFqXcD\nUyvTVwOfLq/3BX5beW/DXpwTjcLfkmTH+W+BK8q87wFfKa+rw5r3Ikdl/ZpSIx3Mv0FNvJP/Kgfo\nwuS9DH8pF4MlyvuN/oPplMDShzRWJku5twDfrsxvBIbGCTSJ0pxBU/PVHLY9pml6El3j1T9W5p0E\nfLlpuVFln3el1Ab68Rk2mioOAU4or1cnA+SaJU+nAFuU91YBNu9DOtUawfJ0jXjal7wjdhLZMXkK\nXX0XHygn7TbN26p70QE+SBkFRwa//yYDwwzKKDWyGed8Ssm4af0+DRpo2sYSZDPiNMpAiMr3fRwZ\nRO6jDBSgKzjvTze1YbKT9c9kk9W2ZE3wd3QN6xxH9jl9lRygcCHwrn7uw5CS15vJWsT1dBWeRlaW\ne3/5rG8Cvtjfz64pD7uS90s1+n1Oo+s+oy3KZ7J2mb6D0hlec9vNhbYVy/F5e9P39T26Rum9mdKC\nQM1zfiD+Bj0DnfRXLmKNKu/OwKnl9Wcp9x6U6caBO5zS3tiHtDYAvkuO6hhDtnlvTI5rH0qWAC8n\nS1K3NV/Yetj2cpUTblQ5ua8iOwm3KReRHci21zvJEt2mZCnms33cn1lGEZFV7KvIduhvlzQatZ5j\nKDdulfc+Qw81phppNi6E3yoXuE3L9HZkU9465EX9UrIgcC29GEnDrEFpDfJCejBZQ5lAlui/Xjle\n7ifbo39dPu+WD3ckS593kjWJHwKXVz8XsiR7MtnftXNlvVXJ2uMHu9nmgmQN8J5y7Iwk+3s+U94f\nRl7MLqSpsNHHfXgXWbM8irwwH0v2q91MpQ+QWUcrjajM7/V9P03f5bpkjeo8cjTcqWT/xOWUobll\nuZl01X5rnfPNx2fZxy/QNRLud3SNXFuOLNTcTV5b7iIfIdTSY6bf39dgZ6BT/siS4Dl03ZS2JFkS\nXpYcFnkZWUW/nLyA79CHg0aUjmyySegwsn9kwXKwfLey7IJkE8ZH+rg/C1JKfmTzy4OV975AXrSX\nJgPHN8lazof7kE5z89M65DDgE8imkTOBp8gOw0aN483AueX1uvSzBEU2n5xJlhDXIAPVbpX0rqPr\nxsN3kDWBUdV9qJlOox9g5/KZ3kE2s91E18CAxsV6BnmxrjUGv5f7uzgZlO6g3ABK3pfwU+Adje+/\n/F+C7H/YprL+CHqoPZIX7gsrn9kUZu1zaUk7Ol3DYBuDLMaQ9xP9jgwcu5IX9KvI2tvrI8jqfm+V\ntKqBYsXy2exIV6f2D8lAtSDZ13QuOWT+u2RAqTX6qptzYhzZzPwlMsj/spwnuzFrLUNl/r7U6Dwf\njL9Bz8Cg7nzX7fVfJMeQDyWbgho34n2hciEYVU7KPfuY1lJkZ+tf6BpJtQnZLPShsv2ryBL33+hF\nlXc26Y0kn7K6EBnwplROjLXJC+y+LTzxG8NT7ygnR6OZaUOyNHs3OfprD7LvoNc1mW5OxPXIktiP\nyBL918synyNrVY0O9aOBvbvZXu0RSWRfwF3lOHlH+d6OAX5RWWYE2Rw2lEp/SHmvFc1Prz+Solx4\nZtB1g9oSZCA8u3n/yOaVXpVWyaa9u8jRf28q398BrThWuknrZ3Td47NAOe/2J0e0nUwf76zv7rOn\na6DJrWTt8wSyMHgnlWYusll2M7K/8DP9OCduIJuyDyJbJK4q6TWatH9BNpXeQSnUdPLfoGdg0Hb8\njVH/RrKZaGXyGT63kzWObRsHWh/T2ZhslricvEB/hq7mmKFkNfUCsuN1LPmcnbe3aB/3o+u5Oh8n\na0iNfpFDy7x+N5WQQeB+8gajJciguyddHcxHkbWyLcvJ0peaTLV02GhP/zJwSHm9K1kS/CBZc7qM\nrFl8n+x0Xqtpe3VrFeuW72ZjcmTRqeRF9M7yfZ5TLmr7kRfYbzBrp2W/h8mSwf8bZP/T0WTwWogM\nkJ+vLDeBDNq7VOatRd4PtFk/vtffAR9txTE5m3TWIwdgNJoRr6Q8ZaBpuX4FXXK00TeBn9PVLDSZ\nbH5btrLcl+kKxH29OXVPcpjsTmX/ni7f3+6VZUaSgfnAvpwTg/E36BkY8B2eNeofTFfU/wkl6pfl\njiOryrWHyXaT1nKUh9eRTSU/JNu176HrgXZ7k81BLX/+C1ltn0n2zSxMtqU3bkJr2dA8si/kpcpJ\nuDf5+IK3lenxZAm1v8MtFyCb8C4jS/lX0/XAwsXIIHh+ORF3ITtn33Dh6UV6I8gSaOPxHWeTTRON\nZ4QdTdbeDiGbL2qNmOllHkaSgfCIso/nkE1gIpv8zqCr6XEEGcyGV9YfSh/6hyrrb0gL7javkc53\ny7F6HtnnVL03pb/3U2xIFghPJ0c63kDXcNahZJPXN8qF++fluFq1n2muTz61dmOyeftiKjUIsslr\nz/7u20D/DXoGBnRn60X9xjDThaiMR+9jeiuWC2Wj7XvzcpE5lhy9cyJZNf5AG/f57ZTHGJAl/Pe2\nKZ0T6bqpaRg5jPVwukbn9OtGLrL9+iyyuWnjctLfWS4CK5dl9iaHKh5AtkOfSTYF9LmPpKT7fTJw\nbEcGpXXJ/q3raHo2GN085qGP6W5P1n43Ie++H0sWbC4qF78vlOWOKZ9B8z0hLbkBcKD+yL7CG6g0\nHbbiYlq+vx8w61OkTy/BofFok/HlmvAdWljSJ2ueF5TXW5Zj5vhyzbmYNjxYsu3f02BnYEB3tl7U\n34MWjWghOye/yax3bE8j24S3IdtqVx6A/f4NlREfbUpjWSqjucjOxKOp+Qytmts/i+xbuoCsuU0i\nmw+uoatj9itkqXtNcvTQES0IVouWNK6ktHOX4HRg03KtCBQjSkD6ZQlS15T9+Bjw1bLMAcBfyRrs\n2sB67T6GBuKPbNa7p7xuScm7fEYPMGuN5T1kTW2LNu/PMmQBpnGj35vJO+b7NQx5UL+jwc7AgO9w\nz1G/X/ceNKWlUpI5hxzBs2y5APSr5tKHfPS7w7VmOvsB97dp24uS4/MfYtY7lJctgenSEuyXKN9n\nn4YHzyH9t5WA8UKb9m9Iyfd15XhpPGxxT7ImdS45Oms42Sf0c/rwlOBO/iv7ti8tuuu9bHMRstD2\nvqb53ydrcC0732eTftvOicH4a8vP+HW444GfStoqIq6TtAPZ0T01In7VyoQiIiSdTQaOL5I/DHRa\nRNzXynRq5OPVAUrqXOC16q+RtXDb/yKHPN4RERcDSDqZ/FXAYyLi2caCks6IiBktTJuIuFPSAWTT\n0Os/zVl+bjX6s21J7yJHON1B3kdyFLCGpLvJfq9NyHsUHqVriOekiHixP+l2moh4iWxGbKV/kYMe\nPiTpOXLgwqeAv5ND6J9scXrNzqXrnIj+HiuDTXN5/vtE0n7Af0XEhAFMc3ngqYh4eaDSnNdIatzc\nNJ582u09ZFv+4+X9oQMRHFsRJJq2txM56GJcRPxv+Q3zxcghsn8qv499CTnMU5E/0TvL70nb7JWf\nSf442RS1BvlUhVYHpvnC/BowhpNV/bOZB6L+/KT8LvbqZEf27xvz5vbvUNKV5M2Vn5G0ItnM9kvg\nRxHxb0kbRMS0smzjfhQHi16QtBjZpPjKYOdlbjVfBgybN8xLF05J65GjeT4YEX+UdBDZSXpURPx1\ncHNnlhwwzDqEpGPJe1e2kzSUfK7Zsz2tZzZQhgx2BszsdacB/5C0BNlU+mypRZl1BNcwzMysFtcw\nzDpMY1iyWadxDcPMzGpxScbMzGpxwDAzs1ocMMzMrBYHDLMmkl6VdFflb1wftrG4pE+2Pndmg8ed\n3mZNJP0rIkb1cxvjgJ9FxDq9XG9Anodl1heuYZjVIGmopBMlTZN0d3mAJZJGSbpB0p2S7pE0qaxy\nPLBaqaGcKOndkn5W2d63JX28vH5E0gmS7gR2kbSapGsk3SHpZklrleV2kXSvpN9LaumTlc3qmB8f\nb27Wk4Ul3VVePxwROwGfAP4ZERuUh1feIunnwGPAThHxnKSlgdskTSF/bXCdiHgLgKR395Dm0xHx\ntrLsDcD+5Um1G5G/BLcF+fsN742ImZIWb+0um/XMAcPsjV5sXOgrtgbWlbRzmV6MfMz648Bx5Tct\nXgPGkD/q1FuXQNZYyF8P/FHlqSDDy/9bgHMlXUr+rrnZgHLAMKtHwMERce0sM7NZaTSwfkS8LOkR\n8vfgm73CrE3Azcu8UP4PAZ7tJmAREfuXGsf2wB2S1o+Ip/uyM2Z94T4Ms3quBQ6QtCCApDUkjSRr\nGk+UYLE5+euNAM+TPw/a8CgwQdLw0py0ZXeJRMRzwMOSdinpqDz6HEmrRcRvI+JL5C/Hrdj63TSb\nPdcwzOo5CxgH3FmeIPsksCNwIfmTv/eQvwv/AEBEPC3pFkn3AldHxKdLU9K9wMPA7+aQ1u7AdyUd\nASxI/tb874ETJY0nazs3lHlmA8bDas3MrBY3SZmZWS0OGGZmVosDhpmZ1eKAYWZmtThgmJlZLQ4Y\nZmZWiwOGmZnV4oBhZma1/D/GW5Kkg96c6wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e76c1b8d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Code here\n",
    "x = range(len(features))\n",
    "y = clf.feature_importances_\n",
    "\n",
    "plt.bar(x,y)\n",
    "# xticks:\n",
    "# first arg - positions on x-axis that will be marked\n",
    "# second arg - Label for the marked points on x-axis\n",
    "# rotation - labels will appear rotated to avoid cluttering\n",
    "plt.xticks(range(len(features)),features,rotation=30)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Feature Importance\")\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Is the relationship between the top 3 most important features (as measured here) negative or positive? If your marketing director asked you to explain the top 3 drivers of churn, how would you interpret the relationship between these 3 features and the churn outcome?  What \"real-life\" connection can you draw between each variable and churn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revenue</th>\n",
       "      <th>eqpdays</th>\n",
       "      <th>outcalls</th>\n",
       "      <th>churndep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>revenue</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.223430</td>\n",
       "      <td>0.504736</td>\n",
       "      <td>-0.020350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eqpdays</th>\n",
       "      <td>-0.223430</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.244088</td>\n",
       "      <td>0.109326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outcalls</th>\n",
       "      <td>0.504736</td>\n",
       "      <td>-0.244088</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.037928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>churndep</th>\n",
       "      <td>-0.020350</td>\n",
       "      <td>0.109326</td>\n",
       "      <td>-0.037928</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           revenue   eqpdays  outcalls  churndep\n",
       "revenue   1.000000 -0.223430  0.504736 -0.020350\n",
       "eqpdays  -0.223430  1.000000 -0.244088  0.109326\n",
       "outcalls  0.504736 -0.244088  1.000000 -0.037928\n",
       "churndep -0.020350  0.109326 -0.037928  1.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code/answer here\n",
    "# Three most important features are - revenue, eqpdays and outcalls\n",
    "# The relationship between these 3 features and churn outcome can be interpreted using correlation between these variables and \n",
    "# churndep\n",
    "train_df[['revenue','eqpdays','outcalls','churndep']].corr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that the revenue and outcalls negatively drive the churn since they are negatively correlated.In real-life it means that person who is making a lot of outcalls or is generating a lot of revenue is less likely to churn(leave the company).On the otherhand eqpdays positively drives the churn since they are positively correlated. It means that the person who has used his equipment for long time(high eqpdays) has higher chances of churning(leaving the company)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Using the classifier built in 2.2, try predicting `\"churndep\"` on both the train_df and test_df data sets. What is the accuracy on each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on training data:  [0 1 1 ..., 0 0 0]\n",
      "Accuracy on training data:  0.999811835544\n",
      "Predictions on test data:  [0 1 1 ..., 1 1 0]\n",
      "Accuracy on test data:  0.531736076267\n"
     ]
    }
   ],
   "source": [
    "# Code here\n",
    "X_train = train_df[features]\n",
    "y_train = train_df[target]\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[target]\n",
    "print(\"Predictions on training data: \", clf.predict(X_train))\n",
    "print(\"Accuracy on training data: \", clf.score(X_train,y_train))\n",
    "print(\"Predictions on test data: \",clf.predict(X_test))\n",
    "print(\"Accuracy on test data: \",clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Finding a Good Decision Tree\n",
    "The default options for your decision tree may not be optimal. We need to analyze whether tuning the parameters can improve the accuracy of the classifier.  For the following options `min_samples_split` and `min_samples_leaf`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Generate a list of 10 values of each for the parameters mim_samples_split and min_samples_leaf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here\n",
    "#????? What values to try ?????\n",
    "\n",
    "min_samples_split_values = np.linspace(2, 100, num=10)\n",
    "min_samples_leaf_values = np.linspace(1, 12, num=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Explain in words your reasoning for choosing the above ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to try out large range of values to see where exactly do we get the best performance. We can also try out with more values of these parameters to do a more extensive search. Once we figure out regions of best perfomance, we can further finetune these parameters by zooming in on the good region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. For each combination of values in 3.1 (there should be 100), build a new classifier and check the classifier's accuracy on the test data. Plot the test set accuracy for these options. Use the values of `min_samples_split` as the x-axis and generate a new series (line) for each of `min_samples_leaf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "min_samples_leaf must be at least 1 or in (0, 0.5], got 2.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-02e7e613e65a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[1;31m# Plot the test score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mserachParams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-02e7e613e65a>\u001b[0m in \u001b[0;36mserachParams\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mtest_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'entropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_samples_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mleaf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mtest_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_samples_split_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\kumar\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    740\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\kumar\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 raise ValueError(\"min_samples_leaf must be at least 1 \"\n\u001b[1;32m    191\u001b[0m                                  \u001b[1;34m\"or in (0, 0.5], got %s\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                                  % self.min_samples_leaf)\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0mmin_samples_leaf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_samples_leaf\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: min_samples_leaf must be at least 1 or in (0, 0.5], got 2.0"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Code here\n",
    "def serachParams():\n",
    "    test_scores = list()\n",
    "    for leaf in min_samples_leaf_values:\n",
    "        for split in min_samples_split_values:\n",
    "            test_scores = list()\n",
    "            clf = tree.DecisionTreeClassifier(criterion='entropy',min_samples_split=split,min_samples_leaf=leaf)\n",
    "            clf.fit(X_train,y_train)\n",
    "            test_scores.append(clf.score(X_test,y_test))\n",
    "        plt.plot(min_samples_split_values, test_scores)\n",
    "        plt.show()\n",
    "\n",
    "# Plot the test score\n",
    "serachParams()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Which configuration returns the best accuracy? What is this accuracy? (Note, if you don't see much variation in the test set accuracy across values of min_samples_split or min_samples_leaf, try redoing the above steps with a different range of values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. If you were working for a marketing department, how would you use your churn production model in a real business environment? Explain why churn prediction might be good for the business and how one might improve churn by using this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
