{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science\n",
    "## Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: Roshan Kumar\n",
    "\n",
    "Student Netid: rk3110\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a Training Set and Training a Decision Tree\n",
    "This is a hands-on task where we build a predictive model using Decision Trees discussed in class. For this part, we will be using the data in `cell2cell_data.csv`.\n",
    "\n",
    "These historical data consist of 39,859 customers: 19,901 customers that churned (i.e., left the company) and 19,958 that did not churn (see the `\"churndep\"` variable). Here are the data set's 11 possible predictor variables for churning behavior: \n",
    "\n",
    "```\n",
    "Pos.  Var. Name  Var. Description\n",
    "----- ---------- --------------------------------------------------------------\n",
    "1     revenue    Mean monthly revenue in dollars\n",
    "2     outcalls   Mean number of outbound voice calls\n",
    "3     incalls    Mean number of inbound voice calls\n",
    "4     months     Months in Service\n",
    "5     eqpdays    Number of days the customer has had his/her current equipment\n",
    "6     webcap     Handset is web capable\n",
    "7     marryyes   Married (1=Yes; 0=No)\n",
    "8     travel     Has traveled to non-US country (1=Yes; 0=No)\n",
    "9     pcown      Owns a personal computer (1=Yes; 0=No)\n",
    "10    creditcd   Possesses a credit card (1=Yes; 0=No)\n",
    "11    retcalls   Number of calls previously made to retention team\n",
    "```\n",
    "\n",
    "The 12th column, the dependent variable `\"churndep\"`, equals 1 if the customer churned, and 0 otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Load the data and prepare it for modeling. Note that the features are already processed for you, so the only thing needed here is split the data into training and testing. Use pandas to create two data frames: train_df and test_df, where train_df has 80% of the data chosen uniformly at random without replacement (test_df should have the other 20%). Also, make sure to write your own code to do the splits. You may use any random() function numpy but DO NOT use the data splitting functions from Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39858\n",
      "         count        mean         std  min     25%     50%     75%      max\n",
      "48.82  31941.0   58.599045   44.080795  0.0   33.49   48.45   70.84   861.11\n",
      "10     31941.0   24.865747   34.679021  0.0    3.00   13.00   33.33   610.33\n",
      "3      31941.0    7.980533   16.432232  0.0    0.00    2.00    8.67   519.33\n",
      "26     31941.0   18.756958    9.605539  6.0   11.00   16.00   24.00    61.00\n",
      "780    31941.0  391.051689  255.547695 -5.0  212.00  341.00  528.00  1823.00\n",
      "0      31941.0    0.896465    0.304661  0.0    1.00    1.00    1.00     1.00\n",
      "0.1    31941.0    0.364140    0.481196  0.0    0.00    0.00    1.00     1.00\n",
      "0.2    31941.0    0.056698    0.231269  0.0    0.00    0.00    0.00     1.00\n",
      "0.3    31941.0    0.184058    0.387538  0.0    0.00    0.00    0.00     1.00\n",
      "1      31941.0    0.674368    0.468618  0.0    0.00    1.00    1.00     1.00\n",
      "4      31941.0    0.043643    0.221791  0.0    0.00    0.00    0.00     4.00\n",
      "1.1    31941.0    0.498763    0.500006  0.0    0.00    0.00    1.00     1.00\n",
      "        count        mean         std   min     25%     50%     75%      max\n",
      "48.82  7917.0   58.775339   44.395562 -5.86   33.03   47.90   71.28   635.06\n",
      "10     7917.0   24.910225   34.798703  0.00    3.00   13.00   33.00   455.00\n",
      "3      7917.0    8.138556   17.391277  0.00    0.00    1.67    8.67   298.33\n",
      "26     7917.0   18.919035    9.550728  6.00   11.00   17.00   24.00    58.00\n",
      "780    7917.0  395.453328  255.823834 -5.00  213.00  346.00  539.00  1693.00\n",
      "0      7917.0    0.891499    0.311031  0.00    1.00    1.00    1.00     1.00\n",
      "0.1    7917.0    0.370216    0.482893  0.00    0.00    0.00    1.00     1.00\n",
      "0.2    7917.0    0.058734    0.235142  0.00    0.00    0.00    0.00     1.00\n",
      "0.3    7917.0    0.190729    0.392901  0.00    0.00    0.00    0.00     1.00\n",
      "1      7917.0    0.682077    0.465699  0.00    0.00    1.00    1.00     1.00\n",
      "4      7917.0    0.044714    0.229289  0.00    0.00    0.00    0.00     4.00\n",
      "1.1    7917.0    0.501326    0.500030  0.00    0.00    1.00    1.00     1.00\n",
      "Index(['48.82', '10', '3', '26', '780', '0', '0.1', '0.2', '0.3', '1', '4',\n",
      "       '1.1'],\n",
      "      dtype='object') \n",
      "\n",
      "        48.82      10       3  26   780  0  0.1  0.2  0.3  1  4  1.1\n",
      "0       83.53   20.00    1.00  31   745  1    0    0    0  0  4    1\n",
      "1       29.99    0.00    0.00  52  1441  0    0    0    1  1  3    1\n",
      "2       51.42    0.00    0.00  36    59  1    0    0    0  0  4    1\n",
      "3       37.75    2.67    0.00  25   572  0    0    0    1  1  3    1\n",
      "4        5.00    0.00    0.00  26   785  1    0    0    0  1  3    1\n",
      "5        5.25    0.00    0.00  45  1354  0    0    0    0  0  2    1\n",
      "6       26.84    4.00    1.33  49  1471  0    1    0    1  1  2    1\n",
      "7       42.71    8.67    0.00  27   224  1    0    0    0  0  3    1\n",
      "8       53.69   15.00    2.33  23   267  1    0    0    0  1  3    1\n",
      "9       33.66    8.33    0.00  31   933  1    0    0    0  0  2    1\n",
      "10      52.56   80.00   31.67  33   402  1    0    0    0  1  3    1\n",
      "11      22.50    8.67    2.67  37   243  1    0    0    0  0  3    1\n",
      "12      98.47   24.67    3.33  35    13  0    0    1    1  1  3    1\n",
      "13       7.82    0.00    0.00  24   561  0    0    0    0  0  2    1\n",
      "14      25.14   15.00    1.00  25   743  1    0    0    0  0  2    1\n",
      "15     131.09   76.67    7.00  25   740  1    1    0    0  1  2    1\n",
      "16      10.00    0.00    0.00  54  1636  0    1    0    0  1  1    1\n",
      "17       5.00    0.00    0.00  53  1584  0    1    1    1  1  1    1\n",
      "18      30.69    1.33    0.00  25   709  1    0    0    0  1  2    1\n",
      "19      49.16    8.67    0.00  47  1434  0    1    0    0  1  1    1\n",
      "20      69.11   18.00    2.67  48  1430  0    0    0    0  1  1    1\n",
      "21      36.16   59.67   13.00  25   733  1    1    0    0  1  2    1\n",
      "22      40.07    3.67    0.00  42  1278  0    0    0    0  0  1    1\n",
      "23      51.22    1.00    0.33  44  1346  0    1    0    0  1  1    1\n",
      "24      37.49    8.00    0.33  40  1209  0    0    0    0  0  1    1\n",
      "25      84.92   67.00    0.33  15   451  1    1    0    0  1  2    1\n",
      "26      12.21    0.00    0.00  35   540  0    1    0    0  1  2    1\n",
      "27     170.92   35.00    0.33  10   302  1    1    0    0  1  2    1\n",
      "28      15.00    1.00    0.33  14   408  1    0    0    0  0  2    1\n",
      "29     199.00   88.67   40.00  19   368  1    0    0    0  0  2    1\n",
      "...       ...     ...     ...  ..   ... ..  ...  ...  ... .. ..  ...\n",
      "39828  237.33  305.00  176.33  31    13  1    0    0    0  0  0    0\n",
      "39829   63.78   18.33   73.33  46   143  1    1    0    0  1  0    0\n",
      "39830   76.47   67.00   32.67  48    25  1    1    0    0  1  0    0\n",
      "39831   97.83   53.33   58.33  52   116  1    1    0    0  1  0    0\n",
      "39832   45.95   23.33   73.33  47    62  1    0    0    0  0  0    0\n",
      "39833   82.18  188.33  116.33  31    81  1    0    0    0  1  0    0\n",
      "39834  140.03  239.00  190.67  24    66  1    0    0    0  0  0    0\n",
      "39835   76.99  290.00  224.33  11   101  1    0    0    0  0  0    0\n",
      "39836   39.54   38.00   26.67  48    26  1    1    0    0  1  0    0\n",
      "39837   83.58   34.00   21.33  54    16  1    1    0    0  1  0    0\n",
      "39838   55.79   11.67   20.00  57    65  1    0    0    0  1  0    0\n",
      "39839  157.86  286.00  282.67  20   262  1    0    0    0  0  0    0\n",
      "39840   56.32   17.00   19.00  52     4  1    0    0    0  1  0    0\n",
      "39841  107.60  217.33  113.33  45   157  1    0    0    0  1  0    0\n",
      "39842   93.02   59.33   23.00  56    10  1    1    0    0  1  0    0\n",
      "39843  107.42  119.33  175.00  42   175  1    0    0    0  0  0    0\n",
      "39844  291.26  389.67  173.00  48   126  1    0    0    0  1  0    0\n",
      "39845  110.33  389.00  218.00  23    92  1    0    0    0  0  0    0\n",
      "39846   51.99  164.00   48.67  49     3  1    0    0    0  1  0    0\n",
      "39847    1.63    0.00    0.00  59    10  1    1    1    1  1  0    0\n",
      "39848   72.96   60.00   86.00  47    68  1    0    0    0  1  0    0\n",
      "39849  226.52   74.67  304.00  17   132  1    0    0    0  0  0    0\n",
      "39850   99.15  251.67  174.33  28    91  1    1    0    0  1  0    0\n",
      "39851  110.73  224.67  184.00  47   267  1    1    0    0  1  0    0\n",
      "39852  185.30  178.67  171.00  41    44  1    1    0    1  1  0    0\n",
      "39853  793.05  437.00  519.33  48   406  1    0    0    0  0  0    0\n",
      "39854  167.59  141.33  283.67  17    28  1    0    0    0  0  0    0\n",
      "39855  151.49  128.67  175.33  47     9  1    1    0    0  1  0    0\n",
      "39856  125.42   90.00  336.67  18    79  1    0    0    0  0  0    0\n",
      "39857  206.67  396.00  404.00  14   126  1    0    0    0  0  0    0\n",
      "\n",
      "[39858 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Code here\n",
    "# load data\n",
    "# Use double slash with U to indicate start of avoid the effect of \\U which is interpreted as a special Unicode character.\n",
    "data = pd.read_csv('C:\\\\Users\\kumar\\OneDrive\\Documents\\GitHub\\DataScienceCourse\\ipython\\hw\\hw_2\\data/cell2cell_data.csv')\n",
    "print(len(data))\n",
    "\n",
    "msk = np.random.rand(len(data)) <= 0.8\n",
    "train = data[msk]\n",
    "test = data[~msk]\n",
    "print(train.describe().transpose())\n",
    "print(test.describe().transpose())\n",
    "print(data.columns,'\\n')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. If we had to, how would we prove to ourselves or a colleague that our data was indeed randomly sampled on X? And by prove, I mean empirically, not just showing this person our code. Don't actually do the work, just describe in your own words a test you could here. Hint: think about this in terms of selection bias and use notes from our 2nd lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Now build and train a decision tree classifier using `DecisionTreeClassifier()` [(manual page)](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) on train_df to predict the `\"churndep\"` target variable. Make sure to use `criterion='entropy'` when instantiating an instance of `DecisionTreeClassifier()`. For all other settings you should use all of the default options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        48.82      10       3  26   780  0  0.1  0.2  0.3  1  4\n",
      "0       83.53   20.00    1.00  31   745  1    0    0    0  0  4\n",
      "2       51.42    0.00    0.00  36    59  1    0    0    0  0  4\n",
      "3       37.75    2.67    0.00  25   572  0    0    0    1  1  3\n",
      "4        5.00    0.00    0.00  26   785  1    0    0    0  1  3\n",
      "5        5.25    0.00    0.00  45  1354  0    0    0    0  0  2\n",
      "6       26.84    4.00    1.33  49  1471  0    1    0    1  1  2\n",
      "7       42.71    8.67    0.00  27   224  1    0    0    0  0  3\n",
      "9       33.66    8.33    0.00  31   933  1    0    0    0  0  2\n",
      "10      52.56   80.00   31.67  33   402  1    0    0    0  1  3\n",
      "11      22.50    8.67    2.67  37   243  1    0    0    0  0  3\n",
      "12      98.47   24.67    3.33  35    13  0    0    1    1  1  3\n",
      "13       7.82    0.00    0.00  24   561  0    0    0    0  0  2\n",
      "15     131.09   76.67    7.00  25   740  1    1    0    0  1  2\n",
      "16      10.00    0.00    0.00  54  1636  0    1    0    0  1  1\n",
      "17       5.00    0.00    0.00  53  1584  0    1    1    1  1  1\n",
      "18      30.69    1.33    0.00  25   709  1    0    0    0  1  2\n",
      "19      49.16    8.67    0.00  47  1434  0    1    0    0  1  1\n",
      "21      36.16   59.67   13.00  25   733  1    1    0    0  1  2\n",
      "22      40.07    3.67    0.00  42  1278  0    0    0    0  0  1\n",
      "23      51.22    1.00    0.33  44  1346  0    1    0    0  1  1\n",
      "25      84.92   67.00    0.33  15   451  1    1    0    0  1  2\n",
      "26      12.21    0.00    0.00  35   540  0    1    0    0  1  2\n",
      "27     170.92   35.00    0.33  10   302  1    1    0    0  1  2\n",
      "28      15.00    1.00    0.33  14   408  1    0    0    0  0  2\n",
      "29     199.00   88.67   40.00  19   368  1    0    0    0  0  2\n",
      "30      16.14    0.00    0.00  44  1324  0    0    0    0  1  1\n",
      "31      61.72   19.33   39.00  16   464  1    0    0    0  0  2\n",
      "32     150.00    1.67    1.00  11   295  1    0    0    1  1  2\n",
      "35      89.58   18.67   12.00  14   417  1    0    0    0  1  2\n",
      "37     121.87   26.33    6.67  11   314  1    1    0    0  1  2\n",
      "...       ...     ...     ...  ..   ... ..  ...  ...  ... .. ..\n",
      "39819   55.51   20.33    1.67  54    45  1    0    0    0  1  0\n",
      "39820  193.69  497.00  227.33  25   176  1    0    0    0  0  0\n",
      "39822   37.28   11.33    3.33  56    84  1    1    0    0  1  0\n",
      "39823   87.52   72.00   30.67  55    99  1    0    0    0  1  0\n",
      "39826   45.85   11.33    0.67  52    11  1    0    0    0  1  0\n",
      "39828  237.33  305.00  176.33  31    13  1    0    0    0  0  0\n",
      "39829   63.78   18.33   73.33  46   143  1    1    0    0  1  0\n",
      "39830   76.47   67.00   32.67  48    25  1    1    0    0  1  0\n",
      "39831   97.83   53.33   58.33  52   116  1    1    0    0  1  0\n",
      "39832   45.95   23.33   73.33  47    62  1    0    0    0  0  0\n",
      "39833   82.18  188.33  116.33  31    81  1    0    0    0  1  0\n",
      "39834  140.03  239.00  190.67  24    66  1    0    0    0  0  0\n",
      "39835   76.99  290.00  224.33  11   101  1    0    0    0  0  0\n",
      "39836   39.54   38.00   26.67  48    26  1    1    0    0  1  0\n",
      "39837   83.58   34.00   21.33  54    16  1    1    0    0  1  0\n",
      "39838   55.79   11.67   20.00  57    65  1    0    0    0  1  0\n",
      "39839  157.86  286.00  282.67  20   262  1    0    0    0  0  0\n",
      "39840   56.32   17.00   19.00  52     4  1    0    0    0  1  0\n",
      "39842   93.02   59.33   23.00  56    10  1    1    0    0  1  0\n",
      "39844  291.26  389.67  173.00  48   126  1    0    0    0  1  0\n",
      "39846   51.99  164.00   48.67  49     3  1    0    0    0  1  0\n",
      "39847    1.63    0.00    0.00  59    10  1    1    1    1  1  0\n",
      "39850   99.15  251.67  174.33  28    91  1    1    0    0  1  0\n",
      "39851  110.73  224.67  184.00  47   267  1    1    0    0  1  0\n",
      "39852  185.30  178.67  171.00  41    44  1    1    0    1  1  0\n",
      "39853  793.05  437.00  519.33  48   406  1    0    0    0  0  0\n",
      "39854  167.59  141.33  283.67  17    28  1    0    0    0  0  0\n",
      "39855  151.49  128.67  175.33  47     9  1    1    0    0  1  0\n",
      "39856  125.42   90.00  336.67  18    79  1    0    0    0  0  0\n",
      "39857  206.67  396.00  404.00  14   126  1    0    0    0  0  0\n",
      "\n",
      "[31959 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# Code here\n",
    "features = ['48.82', '10', '3', '26', '780', '0', '0.1', '0.2', '0.3', '1', '4']\n",
    "target = ['1.1']\n",
    "X = train[features]\n",
    "y = train[target]\n",
    "print(X)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf = clf.fit(X,y) # need to figure out the X and Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Using the resulting model from 2.2, show a bar plot of feature names and their feature importance (hint: check the attributes of the `DecisionTreeClassifier()` object directly in IPython or check the manual!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.25743291  0.19297667  0.13149434  0.08379052  0.25230009  0.0054628\n",
      "  0.02198091  0.00973258  0.0172386   0.02200567  0.00558492]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njTH4JakxBr8kNcbgl6TG/C9nggpsE3lW4QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2626a50f128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Code here\n",
    "x=range(len(features))\n",
    "print(clf.feature_importances_)\n",
    "y=clf.feature_importances_\n",
    "#plt.figure()\n",
    "plt.bar(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Is the relationship between the top 3 most important features (as measured here) negative or positive? If your marketing director asked you to explain the top 3 drivers of churn, how would you interpret the relationship between these 3 features and the churn outcome?  What \"real-life\" connection can you draw between each variable and churn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code/answer here\n",
    "# revenue, outcalls, eqpdays\n",
    "# what does relationship between important features mean?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Using the classifier built in 2.2, try predicting `\"churndep\"` on both the train_df and test_df data sets. What is the accuracy on each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on training data:  [1 1 1 ..., 0 0 0]\n",
      "Accuracy on training data:  0.999812259457\n",
      "Predictions on test data:  [1 1 1 ..., 1 1 1]\n",
      "Accuracy on test data:  0.541840739334\n"
     ]
    }
   ],
   "source": [
    "# Code here\n",
    "X_train = train[features]\n",
    "y_train = train[target]\n",
    "X_test = test[features]\n",
    "y_test = test[target]\n",
    "print(\"Predictions on training data: \", clf.predict(X_train))\n",
    "print(\"Accuracy on training data: \", clf.score(X_train,y_train))\n",
    "print(\"Predictions on test data: \",clf.predict(X_test))\n",
    "print(\"Accuracy on test data: \",clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Finding a Good Decision Tree\n",
    "The default options for your decision tree may not be optimal. We need to analyze whether tuning the parameters can improve the accuracy of the classifier.  For the following options `min_samples_split` and `min_samples_leaf`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Generate a list of 10 values of each for the parameters mim_samples_split and min_samples_leaf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here\n",
    "\n",
    "min_samples_split_values = None\n",
    "min_samples_leaf_values = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Explain in words your reasoning for choosing the above ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. For each combination of values in 3.1 (there should be 100), build a new classifier and check the classifier's accuracy on the test data. Plot the test set accuracy for these options. Use the values of `min_samples_split` as the x-axis and generate a new series (line) for each of `min_samples_leaf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Which configuration returns the best accuracy? What is this accuracy? (Note, if you don't see much variation in the test set accuracy across values of min_samples_split or min_samples_leaf, try redoing the above steps with a different range of values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. If you were working for a marketing department, how would you use your churn production model in a real business environment? Explain why churn prediction might be good for the business and how one might improve churn by using this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
