{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Ratings from reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"C:\\\\Users\\\\kumar\\\\OneDrive\\\\Documents\\\\GitHub\\\\DS-GA-1001-IntroToDataScience\\\\Project\\\\amazon_book_reviews\\\\Andy-Weir-The-Martian.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22570\n",
      "3.0\n",
      "/gp/customer-reviews/R2IP6ZVS4W46VH?ASIN=1491590173\n",
      "Robinson Crusoe on Mars:  Hard SciFi at its best!\n",
      "\"<span class=\"\"a-size-base review-text\"\">Self published books on Amazon are often a crap-shoot.  You read through piles of drek that should never have been published in hope of finding one good book.<br/><br/>This is that book. It is, by far, the best Hard Science Fiction book I've read in years.  In The Martian, Astronaut Mark Watney is accidentally left for dead by his crewmates during a fierce storm.  Marooned on Mars, he has to use his intelligence and wit to survive and wait for a rescue ship, which at best would be over a year away.  The majority of the book is in a 'Log' format, with the main character chronicling what happened during each day.  Periodically the book cuts from the logbook format to a traditional narrative for the supporting characters in space and on Earth.<br/><br/>This is very difficult format to write, but the author manages to pull it off masterfully.  As the main character recounts his triumphs and tragedies you can't help but be dragged into his struggle against the unforgiving Martian environment.  I never thought it would be possible to be kept up past my bedtime by story that involved a man trying to grow potatoes inside an inflatable tent.  The tension and drama are interspersed, where appropriate, by humor and wit.  A good example is when Mark realizes that by growing potatoes he has met the requirements for colonizing Mars (\"\"Take that, Niel Armstrong!\"\").<br/><br/>The author did his homework and it shows.  The science is top notch and well researched.  The only obvious scientific inaccuracy I found was that the effects of a Martian storm were stronger than they are in real life by an order of magnitude.  This is acceptable as without the storm, Mark would not have been abandoned and the story couldn't happen.  The author manages to describe exceptionally complex engineering and chemical processes in a way that is both entertaining and understandable to the layman.<br/><br/>Andy Weir managed to masterfully capture the spirit of NASA during its glory days, when there was no obstacle that could not be solved by an engineer with a slid rule, roll of duct tape, and a dash of creativity.<br/><br/>I bought the Kindle edition, but I hope someday it will be released in paper form.  The hard cover has a place on my bookshelf next to Heinlein, Clarke, and Asimov.</span>\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(filename, sep='\\n')\n",
    "print(len(data))\n",
    "first = data.iloc[10]\n",
    "#print(first) # first is an object [0] gives its content as string\n",
    "print(first[0].split('\\t')[0])  #--first is rating\n",
    "print(first[0].split('\\t')[1])  #--second is Link\n",
    "print(first[0].split('\\t')[2])  #--third is title of the review\n",
    "print(first[0].split('\\t')[3])  #--fourth is the actual review as an HML element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rating                                               link  \\\n",
      "0     4.0  /gp/customer-reviews/RKMO449VT48H3?ASIN=149159...   \n",
      "1     3.0  /gp/customer-reviews/R3RDNZNCOMRN0K?ASIN=14915...   \n",
      "2     4.0  /gp/customer-reviews/R1TC15NPCF9GMW?ASIN=14915...   \n",
      "3     5.0  /gp/customer-reviews/RT3R8XN5KZZGW?ASIN=149159...   \n",
      "4     5.0  /gp/customer-reviews/R32NNLGY7QGRVJ?ASIN=14915...   \n",
      "\n",
      "                          title  \\\n",
      "0            4.7573214851 Stars   \n",
      "1      Who needs nail clippers?   \n",
      "2             Abandoned on Mars   \n",
      "3               Excellent Story   \n",
      "4  Inventive, humorous, tedious   \n",
      "\n",
      "                                              review  \n",
      "0  <span class=\"a-size-base review-text\">I'm a ha...  \n",
      "1  <span class=\"a-size-base review-text\">\"I'm str...  \n",
      "2  <span class=\"a-size-base review-text\">A futuri...  \n",
      "3  <span class=\"a-size-base review-text\">Follow t...  \n",
      "4  <span class=\"a-size-base review-text\">This is ...  \n"
     ]
    }
   ],
   "source": [
    "# load each set of reviews into the same data frame\n",
    "df = None\n",
    "for f in glob.glob('./amazon_book_reviews/*.csv'):\n",
    "    if df is not None: df.append(pd.read_csv(f, sep=\"\\t\", error_bad_lines=False, names=['rating', 'link', 'title', 'review'])\n",
    "                     ,ignore_index=True)\n",
    "    else: df= pd.read_csv(f, sep=\"\\t\", error_bad_lines=False, names=['rating', 'link', 'title', 'review'])\n",
    "\n",
    "print df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98       <span class=\"a-size-base review-text\">It is cl...\n",
      "169      <span class=\"a-size-base review-text\">I wanted...\n",
      "454      <span class=\"a-size-base review-text\">I didn't...\n",
      "851      <span class=\"a-size-base review-text\">Many oth...\n",
      "1393     <span class=\"a-size-base review-text\">I had a ...\n",
      "1433     <span class=\"a-size-base review-text\">The same...\n",
      "1639     <span class=\"a-size-base review-text\">NASA Ast...\n",
      "1866     <span class=\"a-size-base review-text\">I don't ...\n",
      "3278     <span class=\"a-size-base review-text\">While no...\n",
      "3569     <span class=\"a-size-base review-text\">I am mai...\n",
      "3950     <span class=\"a-size-base review-text\">This mov...\n",
      "12351    <span class=\"a-size-base review-text\">What a g...\n",
      "13658    <span class=\"a-size-base review-text\">This is ...\n",
      "15205    <span class=\"a-size-base review-text\">This is ...\n",
      "15464    <span class=\"a-size-base review-text\">I enjoye...\n",
      "15733    <span class=\"a-size-base review-text\">Loved th...\n",
      "16115    <span class=\"a-size-base review-text\">An epic ...\n",
      "16740    <span class=\"a-size-base review-text\">The scie...\n",
      "16872    <span class=\"a-size-base review-text\">Great re...\n",
      "17117    <span class=\"a-size-base review-text\">Great st...\n",
      "17433    <span class=\"a-size-base review-text\">Couldn't...\n",
      "17751    <span class=\"a-size-base review-text\">i bought...\n",
      "17886    <span class=\"a-size-base review-text\">And I co...\n",
      "18708    <span class=\"a-size-base review-text\">Very eng...\n",
      "18788    <span class=\"a-size-base review-text\">The Mart...\n",
      "18797    <span class=\"a-size-base review-text\">This is ...\n",
      "19166    <span class=\"a-size-base review-text\">While Th...\n",
      "19391    <span class=\"a-size-base review-text\">Great bo...\n",
      "19810    <span class=\"a-size-base review-text\">Loved th...\n",
      "20250    <span class=\"a-size-base review-text\">I actual...\n",
      "                               ...                        \n",
      "11757    <span class=\"a-size-base review-text\">Great pr...\n",
      "17022    <span class=\"a-size-base review-text\">It was a...\n",
      "4338     <span class=\"a-size-base review-text\">The freq...\n",
      "18779    <span class=\"a-size-base review-text\">Fun stor...\n",
      "19796    <span class=\"a-size-base review-text\">a little...\n",
      "2847     <span class=\"a-size-base review-text\">The stor...\n",
      "13965    <span class=\"a-size-base review-text\">but a gr...\n",
      "5051     <span class=\"a-size-base review-text\">This is ...\n",
      "2355     <span class=\"a-size-base review-text\">This is ...\n",
      "17972    <span class=\"a-size-base review-text\">fun and ...\n",
      "4676     <span class=\"a-size-base review-text\">I would ...\n",
      "9567     <span class=\"a-size-base review-text\">A seriou...\n",
      "14802    <span class=\"a-size-base review-text\">Very int...\n",
      "7052     <span class=\"a-size-base review-text\">Where to...\n",
      "20395    <span class=\"a-size-base review-text\">Great bo...\n",
      "7636     <span class=\"a-size-base review-text\">Great bo...\n",
      "18041    <span class=\"a-size-base review-text\">Great re...\n",
      "16611    <span class=\"a-size-base review-text\">Very int...\n",
      "12614    <span class=\"a-size-base review-text\">Excellen...\n",
      "9163     <span class=\"a-size-base review-text\">I was am...\n",
      "13244    <span class=\"a-size-base review-text\">The book...\n",
      "2716     <span class=\"a-size-base review-text\">I had to...\n",
      "11578    <span class=\"a-size-base review-text\">I found ...\n",
      "6595     <span class=\"a-size-base review-text\">The ONLY...\n",
      "2944     <span class=\"a-size-base review-text\">\"The Mar...\n",
      "5200     <span class=\"a-size-base review-text\">This boo...\n",
      "20157    <span class=\"a-size-base review-text\">Amazing ...\n",
      "16210    <span class=\"a-size-base review-text\">So Damn ...\n",
      "2258     <span class=\"a-size-base review-text\">I was lo...\n",
      "7739     <span class=\"a-size-base review-text\">Very fun...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Size down data set for closely sized samples of each class\n",
    "#range(1,6)\n",
    "#print df.describe()\n",
    "merge_list = []\n",
    "for r in range(1,6):\n",
    "    var = df.loc[df['rating'] == r]\n",
    "    if r == 3:\n",
    "        var = var.sample(200)\n",
    "    elif r >= 4:\n",
    "        var = var.sample(200)\n",
    "\n",
    "    merge_list.append(var)\n",
    "df =  pd.concat(merge_list)\n",
    "print df.review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98       It is clear that the author really knows his s...\n",
      "169      I wanted to like this book as there is the pot...\n",
      "454      I didn't fact check this book, so I don't know...\n",
      "851      Many other books in this genre ask the age-old...\n",
      "1393     I had a hard read with this guy.  So much cuss...\n",
      "1433     The same way that the new Sherlock reboots Con...\n",
      "1639     NASA Astronaut Mark Watney and his 5 crewmates...\n",
      "1866     I don't read science fiction but my curiosity ...\n",
      "3278     While not my favorite book of all time, this w...\n",
      "3569     I am mainly a fantasy reader, but rarely I fou...\n",
      "3950     This moved to the top of my reading list after...\n",
      "12351    What a great read. The book will restore your ...\n",
      "13658    This is a great combination of a science based...\n",
      "15205    This is the kind of science-based nuts-and-bol...\n",
      "15464    I enjoyed this book at first, but eventually t...\n",
      "15733    Loved the creative tech solutions that hero ne...\n",
      "16115    An epic story of what could happen on a future...\n",
      "16740    The science was really good. Suspense pretty g...\n",
      "16872    Great read,  everyone in my family read it and...\n",
      "17117    Great story, technical strong and at times ver...\n",
      "17433    Couldn't wait to find out what would happen next!\n",
      "17751    i bought thi on my travels in an airport rando...\n",
      "17886    And I couldn't stop reading once I started.  P...\n",
      "18708    Very engaging read. Great scientific detail bu...\n",
      "18788    The Martian is like a Tom Clancy novel. Real w...\n",
      "18797    This is a great update to the Robinson Caruso ...\n",
      "19166    While The Martian's concept has promise and ho...\n",
      "19391                              Great book of survival.\n",
      "19810                                     Loved this book!\n",
      "20250    I actually liked the movie better than the boo...\n",
      "                               ...                        \n",
      "11757    Great premise , lots of technical stuff but ma...\n",
      "17022    It was an awesome book. Every book lover shoul...\n",
      "4338     The frequent math calculations put a little to...\n",
      "18779                     Fun story. Enjoyed it very much!\n",
      "19796                                    a little pedantic\n",
      "2847     The story reads like a typical hollywoodish ad...\n",
      "13965    but a great premise and an easy read, I really...\n",
      "5051     This is one of my all time favorite books. Req...\n",
      "2355     This is a wonderful book covering the unexpect...\n",
      "17972           fun and suspenseful read to the last page.\n",
      "4676     I would recommend this book not only to anyone...\n",
      "9567     A seriously funny book. I like how it doesn't ...\n",
      "14802    Very interesting plot. The Science seemed prob...\n",
      "7052     Where to begin. I had no idea this was the aut...\n",
      "20395                                          Great book!\n",
      "7636     Great book. As a 90's \"kid\" it feels a bit lik...\n",
      "18041            Great read,  looking forward to the movie\n",
      "16611    Very interesting science but it got in the way...\n",
      "12614    Excellent writing. The story makes science shi...\n",
      "9163     I was amazed at how interesting this story was...\n",
      "13244    The book kept me on my toes, was well written,...\n",
      "2716     I had to get used to the rather goofy behavior...\n",
      "11578    I found this a great read, fun and not too hea...\n",
      "6595     The ONLY drawback to this book; it's going to ...\n",
      "2944     \"The Martian\" is easily the best book I've rea...\n",
      "5200     This book is like a very high tech version of ...\n",
      "20157                                         Amazing book\n",
      "16210    So Damn good. I want to keep reading it  pleas...\n",
      "2258     I was looking for something new to read and Am...\n",
      "7739     Very fun read. Watney is funny and resourceful...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Get rid of HTML tags using HTMLParser\n",
    "from HTMLParser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "df['review'] = df['review'].apply(strip_tags)\n",
    "print df.review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Adding title\n",
    "# df['review'] += ' ' + df['title']\n",
    "\n",
    "# Create simple classification set, 5 star, 4 star, less than 3 star. \n",
    "#class1 = df.loc[df['rating'] >= 3]\n",
    "#class0 = df.loc[df['rating'] <=2] \n",
    "\n",
    "#class1['rating'] = 1\n",
    "#class0['rating'] = 0\n",
    "\n",
    "#df = pd.concat([class1, class0])\n",
    "\n",
    "shuff_df = df.sample(frac=1)\n",
    "cut = int(len(shuff_df)*3/4)\n",
    "train_df = shuff_df[:cut]\n",
    "test_df = shuff_df[cut:]\n",
    "amazonbook = {'train': train_df, 'test': test_df}\n",
    "with open('amazonbook.p', 'wb') as f:\n",
    "    pickle.dump(amazonbook, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       rating                                               link  \\\n",
      "20509       1  /gp/customer-reviews/R3VHDH367Y8NUS?ASIN=14915...   \n",
      "552         1  /gp/customer-reviews/R1DKSZCZAGFX4K?ASIN=14915...   \n",
      "22363       0  /gp/customer-reviews/R1H5HLH7KCZKES?ASIN=14915...   \n",
      "16580       0  /gp/customer-reviews/R7XHEJNF7ATUN?ASIN=149159...   \n",
      "20051       0  /gp/customer-reviews/R34WW3FYF4ORM0?ASIN=14915...   \n",
      "\n",
      "                               title  \\\n",
      "20509                     Five Stars   \n",
      "552             well worth your time   \n",
      "22363  Sorry  not fast enough ACTION   \n",
      "16580                 Saw the Movie!   \n",
      "20051                Liked the story   \n",
      "\n",
      "                                                  review  \n",
      "20509                                         Great book  \n",
      "552    This is a fascinating read, the plot driven by...  \n",
      "22363  I like the details of how he did what he did a...  \n",
      "16580  Saw the movie and wanted to see if the movie f...  \n",
      "20051  Very entertaining and fast-paced book, but the...  \n"
     ]
    }
   ],
   "source": [
    "#print train_df.head()\n",
    "print train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(841L,)\n",
      "[ 2.  3.  3.  2.  1.  1.  2.  3.  4.  4.  1.  0.  3.  1.  2.  0.  3.  4.\n",
      "  2.  1.  0.  3.  3.  2.  4.  1.  1.  0.  1.  3.  1.  3.  3.  3.  0.  1.\n",
      "  3.  4.  1.  0.  2.  3.  3.  0.  0.  3.  4.  0.  0.  4.  1.  3.  3.  1.\n",
      "  0.  4.  3.  1.  4.  4.  0.  4.  1.  3.  0.  0.  4.  3.  4.  0.  0.  0.\n",
      "  1.  2.  0.  1.  0.  0.  1.  3.  2.  0.  1.  0.  1.  2.  3.  2.  3.  3.\n",
      "  0.  1.  2.  3.  4.  1.  1.  3.  0.  0.  3.  1.  0.  4.  0.  0.  2.  2.\n",
      "  4.  4.  2.  1.  3.  0.  1.  3.  1.  3.  0.  0.  1.  0.  0.  1.  2.  2.\n",
      "  2.  2.  3.  2.  0.  0.  3.  1.  4.  1.  0.  1.  1.  3.  3.  0.  0.  3.\n",
      "  2.  2.  0.  1.  0.  1.  2.  2.  4.  1.  0.  1.  0.  1.  1.  0.  0.  1.\n",
      "  2.  2.  0.  0.  4.  1.  0.  4.  2.  2.  1.  2.  1.  1.  0.  0.  0.  1.\n",
      "  4.  3.  2.  2.  4.  2.  0.  2.  1.  1.  4.  2.  0.  4.  2.  4.  2.  4.\n",
      "  0.  0.  2.  0.  2.  1.  1.  1.  2.  2.  1.  4.  3.  3.  0.  0.  4.  1.\n",
      "  1.  4.  3.  4.  4.  2.  3.  4.  4.  4.  1.  4.  1.  4.  2.  2.  1.  0.\n",
      "  0.  3.  1.  3.  2.  3.  0.  3.  2.  4.  1.  2.  0.  1.  3.  3.  0.  1.\n",
      "  1.  1.  2.  0.  1.  4.  0.  3.  2.  1.  1.  3.  3.  1.  3.  1.  3.  2.\n",
      "  0.  1.  3.  4.  1.  2.  3.  4.  4.  1.  2.  2.  4.  3.  2.  1.  4.  0.\n",
      "  4.  3.  2.  1.  2.  3.  0.  0.  0.  1.  0.  2.  1.  0.  3.  2.  4.  2.\n",
      "  1.  4.  3.  1.  0.  2.  1.  3.  4.  3.  0.  4.  1.  3.  3.  3.  1.  0.\n",
      "  4.  3.  1.  4.  0.  3.  1.  4.  3.  1.  1.  1.  1.  2.  0.  0.  3.  0.\n",
      "  4.  0.  3.  3.  1.  1.  2.  1.  1.  0.  2.  1.  4.  4.  4.  0.  4.  0.\n",
      "  3.  3.  2.  4.  0.  1.  3.  0.  4.  4.  0.  0.  1.  1.  2.  3.  1.  1.\n",
      "  0.  4.  0.  4.  1.  1.  1.  3.  4.  4.  2.  0.  0.  1.  2.  0.  0.  1.\n",
      "  4.  1.  3.  4.  1.  2.  2.  2.  4.  0.  2.  0.  4.  3.  3.  3.  1.  1.\n",
      "  1.  1.  0.  1.  3.  0.  3.  1.  2.  4.  0.  4.  3.  4.  3.  0.  1.  3.\n",
      "  4.  4.  3.  3.  2.  1.  3.  4.  2.  4.  4.  2.  4.  2.  2.  0.  1.  4.\n",
      "  2.  0.  0.  3.  2.  0.  3.  4.  4.  0.  1.  3.  2.  0.  3.  3.  4.  0.\n",
      "  2.  0.  0.  2.  3.  2.  1.  1.  0.  0.  2.  1.  4.  2.  2.  0.  4.  3.\n",
      "  0.  4.  0.  4.  3.  2.  4.  4.  3.  4.  2.  0.  2.  1.  0.  0.  0.  1.\n",
      "  1.  1.  0.  4.  0.  4.  1.  1.  2.  4.  2.  3.  1.  4.  3.  1.  4.  2.\n",
      "  3.  2.  2.  0.  1.  0.  2.  3.  0.  0.  2.  4.  2.  1.  4.  4.  1.  2.\n",
      "  2.  1.  3.  3.  4.  4.  0.  2.  1.  3.  3.  1.  2.  1.  3.  4.  1.  2.\n",
      "  3.  0.  0.  2.  2.  2.  0.  1.  3.  4.  1.  4.  4.  1.  4.  1.  1.  1.\n",
      "  4.  0.  1.  2.  1.  4.  1.  0.  4.  0.  2.  1.  0.  4.  1.  4.  3.  1.\n",
      "  0.  4.  0.  3.  3.  1.  2.  0.  0.  1.  0.  2.  4.  3.  4.  1.  2.  1.\n",
      "  0.  0.  1.  1.  2.  1.  1.  1.  1.  1.  1.  3.  0.  4.  2.  1.  0.  2.\n",
      "  1.  0.  2.  0.  1.  4.  4.  0.  2.  4.  4.  1.  0.  0.  1.  3.  2.  0.\n",
      "  0.  3.  0.  0.  1.  1.  0.  0.  0.  1.  3.  4.  1.  1.  1.  2.  3.  1.\n",
      "  3.  4.  2.  1.  1.  1.  2.  4.  4.  2.  4.  3.  2.  4.  1.  2.  0.  0.\n",
      "  3.  0.  3.  0.  4.  0.  4.  1.  4.  4.  4.  1.  1.  3.  4.  3.  3.  2.\n",
      "  4.  2.  0.  1.  2.  1.  1.  3.  0.  3.  4.  3.  1.  0.  0.  3.  0.  2.\n",
      "  3.  3.  0.  2.  1.  3.  0.  4.  0.  4.  4.  1.  0.  2.  3.  1.  4.  3.\n",
      "  2.  3.  4.  2.  0.  0.  2.  2.  2.  2.  2.  3.  1.  0.  4.  1.  0.  3.\n",
      "  0.  1.  4.  2.  2.  1.  2.  4.  0.  3.  0.  1.  3.  1.  1.  3.  3.  2.\n",
      "  0.  0.  3.  1.  2.  4.  1.  4.  3.  2.  4.  1.  2.  3.  1.  2.  4.  4.\n",
      "  4.  4.  1.  1.  3.  0.  4.  1.  4.  3.  1.  0.  0.  4.  1.  1.  4.  1.\n",
      "  4.  1.  1.  1.  3.  0.  3.  0.  0.  4.  1.  2.  2.  3.  3.  1.  3.  3.\n",
      "  0.  1.  3.  0.  3.  1.  4.  0.  2.  4.  1.  2.  3.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# load data, format specific to NB\n",
    "with open('amazonbook.p', 'rb') as f:\n",
    "    amazonbook = pickle.load(f)\n",
    "\n",
    "train = {'data': amazonbook['train']['review'], 'target': amazonbook['train']['rating']}\n",
    "test = {'data': amazonbook['test']['review'], 'target': amazonbook['test']['rating']}\n",
    "\n",
    "print train['data'].shape\n",
    "print train['target'].values-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "def zeroOne(y,a) :\n",
    "    '''\n",
    "    Computes the zero-one loss.\n",
    "    @param y: output class\n",
    "    @param a: predicted class\n",
    "    @return 1 if different, 0 if same\n",
    "    '''\n",
    "    return int(y != a)\n",
    "\n",
    "def featureMap(X,y,num_classes) :\n",
    "    '''\n",
    "    Computes the class-sensitive features.\n",
    "    @param X: array-like, shape = [n_samples,n_inFeatures] or [n_inFeatures,], input features for input data\n",
    "    @param y: a target class (in range 0,..,num_classes-1)\n",
    "    @return array-like, shape = [n_samples,n_outFeatures], the class sensitive features for class y\n",
    "    '''\n",
    "    #The following line handles X being a 1d-array or a 2d-array\n",
    "    num_samples, num_inFeatures = (1,X.shape[0]) if len(X.shape) == 1 else (X.shape[0],X.shape[1])\n",
    "    #your code goes here, and replaces following return\n",
    "    num_outFeatures = num_classes*num_inFeatures\n",
    "    #feature_map = np.zeros(num_outFeatures)\n",
    "    featurized_X = np.zeros(num_samples*num_outFeatures).reshape(num_samples,num_outFeatures)\n",
    "    #Create a method for num_samples == 1\n",
    "    if num_samples == 1:\n",
    "        feature_map = np.zeros(num_outFeatures)\n",
    "        feature_map[y*num_inFeatures:(y+1)*num_inFeatures]=X\n",
    "        return feature_map\n",
    "    # Compute feature for each sample\n",
    "    for idx,sample in enumerate(X):\n",
    "        yi = y[idx]\n",
    "        feature_map = np.zeros(num_outFeatures)\n",
    "        feature_map[yi*num_inFeatures:(yi+1)*num_inFeatures]=sample\n",
    "        featurized_X[idx,:] = feature_map\n",
    "    return featurized_X\n",
    "\n",
    "def sgd(X, y, num_outFeatures, subgd, eta = 0.1, T = 10000):\n",
    "    '''\n",
    "    Runs subgradient descent, and outputs resulting parameter vector.\n",
    "    @param X: array-like, shape = [n_samples,n_features], input training data \n",
    "    @param y: array-like, shape = [n_samples,], class labels\n",
    "    @param num_outFeatures: number of class-sensitive features\n",
    "    @param subgd: function taking x,y and giving subgradient of objective\n",
    "    @param eta: learning rate for SGD\n",
    "    @param T: maximum number of iterations\n",
    "    @return: vector of weights\n",
    "    '''\n",
    "    num_samples = X.shape[0]\n",
    "    #your code goes here and replaces following return statement\n",
    "    # initilize w\n",
    "    decay = 1\n",
    "    w = np.zeros(num_outFeatures)\n",
    "    for t in range(T):\n",
    "        #caluclate subgradient\n",
    "        for idx,xi in enumerate(X):\n",
    "            eta = eta/decay\n",
    "            decay = decay + 1\n",
    "            sg =subgd(xi,y[idx],w) \n",
    "            w = w - eta*sg\n",
    "    return w\n",
    "\n",
    "class MulticlassSVM(BaseEstimator, ClassifierMixin):\n",
    "    '''\n",
    "    Implements a Multiclass SVM estimator.\n",
    "    '''\n",
    "    def __init__(self, num_outFeatures, lam=1.0, num_classes=3, Delta=zeroOne, Psi=featureMap):       \n",
    "        '''\n",
    "        Creates a MulticlassSVM estimator.\n",
    "        @param num_outFeatures: number of class-sensitive features produced by Psi\n",
    "        @param lam: l2 regularization parameter\n",
    "        @param num_classes: number of classes (assumed numbered 0,..,num_classes-1)\n",
    "        @param Delta: class-sensitive loss function taking two arguments (i.e., target margin)\n",
    "        @param Psi: class-sensitive feature map taking two arguments\n",
    "        '''\n",
    "        self.num_outFeatures = num_outFeatures\n",
    "        self.lam = lam\n",
    "        self.num_classes = num_classes\n",
    "        self.Delta = Delta\n",
    "        self.Psi = lambda X,y : Psi(X,y,num_classes)\n",
    "        self.fitted = False\n",
    "    \n",
    "    def subgradient(self,x,y,w):\n",
    "        '''\n",
    "        Computes the subgradient at a given data point x,y\n",
    "        @param x: sample input\n",
    "        @param y: sample class\n",
    "        @param w: parameter vector\n",
    "        @return returns subgradient vector at given x,y,w\n",
    "        '''\n",
    "        #Your code goes here and replaces the following return statement\n",
    "        # Compute (class-sensitive-loss + margin) for each class\n",
    "        si = self.Psi\n",
    "        loss  = [self.Delta(y,cls) + np.dot(w, (si(x,cls) - si(x,y))) for cls in range(self.num_classes)] \n",
    "        # get the class which maximizes loss(so that we have an upper bound on o/1 loss)\n",
    "        y_opt = np.argmax(loss)\n",
    "        # Using graddient expression derived in 3.3\n",
    "        return 2*self.lam*w + si(x,y_opt)-si(x,y)\n",
    "\n",
    "    def fit(self,X,y,eta=0.1,T=1):\n",
    "        '''\n",
    "        Fits multiclass SVM\n",
    "        @param X: array-like, shape = [num_samples,num_inFeatures], input data\n",
    "        @param y: array-like, shape = [num_samples,], input classes\n",
    "        @param eta: learning rate for SGD\n",
    "        @param T: maximum number of iterations\n",
    "        @return returns self\n",
    "        '''\n",
    "        self.coef_ = sgd(X,y,self.num_outFeatures,self.subgradient,eta,T)\n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        '''\n",
    "        Returns the score on each input for each class. Assumes\n",
    "        that fit has been called.\n",
    "        @param X : array-like, shape = [n_samples, n_inFeatures]\n",
    "        @return array-like, shape = [n_samples, n_classes] giving scores for each sample,class pairing\n",
    "        '''\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data.\")\n",
    "        print(X.shape)\n",
    "        #Your code goes here and replaces following return statement\n",
    "        # Initialize the score_matrix of appropriate dimensions\n",
    "        score_matrix = np.zeros(len(X)*self.num_classes).reshape(len(X), self.num_classes)\n",
    "        # Compute score for each sample and for each class\n",
    "        for i,x_i in enumerate(X):\n",
    "            score_matrix[i,:] = [np.dot(self.coef_, self.Psi(x_i,cls)) for cls in range(self.num_classes)]\n",
    "        return score_matrix\n",
    "            \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict the class with the highest score.\n",
    "        @param X: array-like, shape = [n_samples, n_inFeatures], input data to predict\n",
    "        @return array-like, shape = [n_samples,], class labels predicted for each data point\n",
    "        '''\n",
    "\n",
    "        #Your code goes here and replaces following return statement\n",
    "        pred = np.zeros(X.shape[0])\n",
    "        score_matrix = self.decision_function(X)\n",
    "        for idx,row in enumerate(score_matrix):\n",
    "            pred[idx] = np.argmax(row)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(841, 4900)\n",
      "(841, 4900)\n",
      "(281, 2909)\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_count = count_vect.fit_transform(train['data'])\n",
    "print X_train_count.shape # 841,4900\n",
    "# Working on Improving the features\n",
    "# Better than counts are the frequencies of occurence of words\n",
    "# Better than frequencies are tF-idf(term frequency times inverse Document Frequency)\n",
    "# Count can be converted to tf-idf with standard sklearn package\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_count)\n",
    "print(X_train_tfidf.shape)\n",
    "\n",
    "X_test_count = count_vect.fit_transform(test['data'])\n",
    "X_test_tfidf = tfidf_transformer.fit_transform(X_test_count)\n",
    "print X_test_tfidf.shape\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-eae6f847e150>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;31m#X_train = (X - np.mean(X,axis=0))/np.std(X,axis=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m  \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"w:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-754a868572e1>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, eta, T)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[1;32mreturn\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         '''\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_outFeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubgradient\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfitted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-754a868572e1>\u001b[0m in \u001b[0;36msgd\u001b[0;34m(X, y, num_outFeatures, subgd, eta, T)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0meta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdecay\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mdecay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecay\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0msg\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0msubgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-754a868572e1>\u001b[0m in \u001b[0;36msubgradient\u001b[0;34m(self, x, y, w)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[1;31m# Compute (class-sensitive-loss + margin) for each class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0msi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPsi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mloss\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[1;31m# get the class which maximizes loss(so that we have an upper bound on o/1 loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0my_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-754a868572e1>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPsi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mPsi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfitted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-754a868572e1>\u001b[0m in \u001b[0;36mfeatureMap\u001b[0;34m(X, y, num_classes)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mfeature_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_outFeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mfeature_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnum_inFeatures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnum_inFeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfeature_map\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[1;31m# Compute feature for each sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "est = MulticlassSVM(4120900,lam=1)\n",
    "#X_train = (X - np.mean(X,axis=0))/np.std(X,axis=0)\n",
    "y=  train['target'].values-1\n",
    "est.fit(X_train_tfidf,y)\n",
    "print(\"w:\")\n",
    "print(est.coef_)\n",
    "Z = est.predict(X_test)\n",
    "print Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.316725978648\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize       \n",
    "from sklearn import svm\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class Tokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "# Tokenize the text\n",
    "t = Tokenizer()\n",
    "\n",
    "# Setting up the pipeline\n",
    "text_clf = Pipeline([('vect', TfidfVectorizer(tokenizer=t, ngram_range=(1, 2), binary=True)),\n",
    "                     ('clf', OneVsRestClassifier(svm.LinearSVC(loss='hinge', fit_intercept=False, C=0.1))),\n",
    "])\n",
    "\n",
    "# train the model\n",
    "text_clf = text_clf.fit(train['data'], train['target'])\n",
    "\n",
    "#predict\n",
    "predicted = text_clf.predict(test['data'])\n",
    "#print type(predicted)\n",
    "#print test['target'].values\n",
    "print np.mean(predicted == test['target'].values)\n",
    "#print metrics.classification_report(test['target'], predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
