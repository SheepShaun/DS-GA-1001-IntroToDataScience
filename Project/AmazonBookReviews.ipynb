{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Ratings from reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(182451L,)\n",
      "[ 3.  4.  3. ...,  4.  3.  1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# load data, format specific to NB\n",
    "with open('amazonbook.p', 'rb') as f:\n",
    "    amazonbook = pickle.load(f)\n",
    "\n",
    "train = {'data': amazonbook['train']['review'], 'target': amazonbook['train']['rating']}\n",
    "test = {'data': amazonbook['test']['review'], 'target': amazonbook['test']['rating']}\n",
    "\n",
    "print train['data'].shape\n",
    "print train['target'].values-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "def zeroOne(y,a) :\n",
    "    '''\n",
    "    Computes the zero-one loss.\n",
    "    @param y: output class\n",
    "    @param a: predicted class\n",
    "    @return 1 if different, 0 if same\n",
    "    '''\n",
    "    return int(y != a)\n",
    "\n",
    "def featureMap(X,y,num_classes) :\n",
    "    '''\n",
    "    Computes the class-sensitive features.\n",
    "    @param X: array-like, shape = [n_samples,n_inFeatures] or [n_inFeatures,], input features for input data\n",
    "    @param y: a target class (in range 0,..,num_classes-1)\n",
    "    @return array-like, shape = [n_samples,n_outFeatures], the class sensitive features for class y\n",
    "    '''\n",
    "    #The following line handles X being a 1d-array or a 2d-array\n",
    "    num_samples, num_inFeatures = (1,X.shape[0]) if len(X.shape) == 1 else (X.shape[0],X.shape[1])\n",
    "    #your code goes here, and replaces following return\n",
    "    num_outFeatures = num_classes*num_inFeatures\n",
    "    #feature_map = np.zeros(num_outFeatures)\n",
    "    featurized_X = np.zeros(num_samples*num_outFeatures).reshape(num_samples,num_outFeatures)\n",
    "    #Create a method for num_samples == 1\n",
    "    if num_samples == 1:\n",
    "        feature_map = np.zeros(num_outFeatures)\n",
    "        feature_map[y*num_inFeatures:(y+1)*num_inFeatures]=X\n",
    "        return feature_map\n",
    "    # Compute feature for each sample\n",
    "    for idx,sample in enumerate(X):\n",
    "        yi = y[idx]\n",
    "        feature_map = np.zeros(num_outFeatures)\n",
    "        feature_map[yi*num_inFeatures:(yi+1)*num_inFeatures]=sample\n",
    "        featurized_X[idx,:] = feature_map\n",
    "    return featurized_X\n",
    "\n",
    "def sgd(X, y, num_outFeatures, subgd, eta = 0.1, T = 1):\n",
    "    '''\n",
    "    Runs subgradient descent, and outputs resulting parameter vector.\n",
    "    @param X: array-like, shape = [n_samples,n_features], input training data \n",
    "    @param y: array-like, shape = [n_samples,], class labels\n",
    "    @param num_outFeatures: number of class-sensitive features\n",
    "    @param subgd: function taking x,y and giving subgradient of objective\n",
    "    @param eta: learning rate for SGD\n",
    "    @param T: maximum number of iterations\n",
    "    @return: vector of weights\n",
    "    '''\n",
    "    num_samples = X.shape[0]\n",
    "    #your code goes here and replaces following return statement\n",
    "    # initilize w\n",
    "    decay = 1\n",
    "    w = np.zeros(num_outFeatures)\n",
    "    for t in range(T):\n",
    "        #caluclate subgradient\n",
    "        for idx,xi in enumerate(X):\n",
    "            eta = eta/decay\n",
    "            decay = decay + 1\n",
    "            sg =subgd(xi,y[idx],w) \n",
    "            w = w - eta*sg\n",
    "    return w\n",
    "\n",
    "class MulticlassSVM(BaseEstimator, ClassifierMixin):\n",
    "    '''\n",
    "    Implements a Multiclass SVM estimator.\n",
    "    '''\n",
    "    def __init__(self, num_outFeatures, lam=1.0, num_classes=5, Delta=zeroOne, Psi=featureMap):       \n",
    "        '''\n",
    "        Creates a MulticlassSVM estimator.\n",
    "        @param num_outFeatures: number of class-sensitive features produced by Psi\n",
    "        @param lam: l2 regularization parameter\n",
    "        @param num_classes: number of classes (assumed numbered 0,..,num_classes-1)\n",
    "        @param Delta: class-sensitive loss function taking two arguments (i.e., target margin)\n",
    "        @param Psi: class-sensitive feature map taking two arguments\n",
    "        '''\n",
    "        self.num_outFeatures = num_outFeatures\n",
    "        self.lam = lam\n",
    "        self.num_classes = num_classes\n",
    "        self.Delta = Delta\n",
    "        self.Psi = lambda X,y : Psi(X,y,num_classes)\n",
    "        self.fitted = False\n",
    "    \n",
    "    def subgradient(self,x,y,w):\n",
    "        '''\n",
    "        Computes the subgradient at a given data point x,y\n",
    "        @param x: sample input\n",
    "        @param y: sample class\n",
    "        @param w: parameter vector\n",
    "        @return returns subgradient vector at given x,y,w\n",
    "        '''\n",
    "        #Your code goes here and replaces the following return statement\n",
    "        # Compute (class-sensitive-loss + margin) for each class\n",
    "        si = self.Psi\n",
    "        loss  = [self.Delta(y,cls) + np.dot(w, (si(x,cls) - si(x,y))) for cls in range(self.num_classes)] \n",
    "        # get the class which maximizes loss(so that we have an upper bound on o/1 loss)\n",
    "        y_opt = np.argmax(loss)\n",
    "        # Using graddient expression derived in 3.3\n",
    "        return 2*self.lam*w + si(x,y_opt)-si(x,y)\n",
    "\n",
    "    def fit(self,X,y,eta=0.1,T=1):\n",
    "        '''\n",
    "        Fits multiclass SVM\n",
    "        @param X: array-like, shape = [num_samples,num_inFeatures], input data\n",
    "        @param y: array-like, shape = [num_samples,], input classes\n",
    "        @param eta: learning rate for SGD\n",
    "        @param T: maximum number of iterations\n",
    "        @return returns self\n",
    "        '''\n",
    "        self.coef_ = sgd(X,y,self.num_outFeatures,self.subgradient,eta,T)\n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        '''\n",
    "        Returns the score on each input for each class. Assumes\n",
    "        that fit has been called.\n",
    "        @param X : array-like, shape = [n_samples, n_inFeatures]\n",
    "        @return array-like, shape = [n_samples, n_classes] giving scores for each sample,class pairing\n",
    "        '''\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data.\")\n",
    "        print(X.shape)\n",
    "        #Your code goes here and replaces following return statement\n",
    "        # Initialize the score_matrix of appropriate dimensions\n",
    "        score_matrix = np.zeros(len(X)*self.num_classes).reshape(len(X), self.num_classes)\n",
    "        # Compute score for each sample and for each class\n",
    "        for i,x_i in enumerate(X):\n",
    "            score_matrix[i,:] = [np.dot(self.coef_, self.Psi(x_i,cls)) for cls in range(self.num_classes)]\n",
    "        return score_matrix\n",
    "            \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict the class with the highest score.\n",
    "        @param X: array-like, shape = [n_samples, n_inFeatures], input data to predict\n",
    "        @return array-like, shape = [n_samples,], class labels predicted for each data point\n",
    "        '''\n",
    "\n",
    "        #Your code goes here and replaces following return statement\n",
    "        pred = np.zeros(X.shape[0])\n",
    "        score_matrix = self.decision_function(X)\n",
    "        for idx,row in enumerate(score_matrix):\n",
    "            pred[idx] = np.argmax(row)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81238, 40159)\n",
      "(81238, 40159)\n",
      "(27080, 24373)\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_count = count_vect.fit_transform(train['data'])\n",
    "print X_train_count.shape # 841,4900\n",
    "# Working on Improving the features\n",
    "# Better than counts are the frequencies of occurence of words\n",
    "# Better than frequencies are tF-idf(term frequency times inverse Document Frequency)\n",
    "# Count can be converted to tf-idf with standard sklearn package\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_count)\n",
    "print(X_train_tfidf.shape)\n",
    "\n",
    "X_test_count = count_vect.fit_transform(test['data'])\n",
    "X_test_tfidf = tfidf_transformer.fit_transform(X_test_count)\n",
    "print X_test_tfidf.shape\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-151bfe243e87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;31m#X_train = (X - np.mean(X,axis=0))/np.std(X,axis=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m  \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"w:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-4e73acc09fa1>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, eta, T)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[1;32mreturn\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         '''\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_outFeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubgradient\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfitted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-4e73acc09fa1>\u001b[0m in \u001b[0;36msgd\u001b[0;34m(X, y, num_outFeatures, subgd, eta, T)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0meta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdecay\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mdecay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecay\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0msg\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0msubgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-4e73acc09fa1>\u001b[0m in \u001b[0;36msubgradient\u001b[0;34m(self, x, y, w)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[1;31m# Compute (class-sensitive-loss + margin) for each class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0msi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPsi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mloss\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[1;31m# get the class which maximizes loss(so that we have an upper bound on o/1 loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0my_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-4e73acc09fa1>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPsi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mPsi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfitted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-4e73acc09fa1>\u001b[0m in \u001b[0;36mfeatureMap\u001b[0;34m(X, y, num_classes)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mfeature_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_outFeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mfeature_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnum_inFeatures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnum_inFeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfeature_map\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[1;31m# Compute feature for each sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "est = MulticlassSVM(200795,lam=1)\n",
    "#X_train = (X - np.mean(X,axis=0))/np.std(X,axis=0)\n",
    "y=  train['target'].values-1\n",
    "est.fit(X_train_tfidf,y)\n",
    "print(\"w:\")\n",
    "print(est.coef_)\n",
    "#Z = est.predict(X_test)\n",
    "#print Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OneVsRest Classifier with LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.538541221349\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize       \n",
    "from sklearn import svm\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class Tokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "# Tokenize the text\n",
    "t = Tokenizer()\n",
    "\n",
    "# Setting up the pipeline\n",
    "text_clf = Pipeline([('vect', TfidfVectorizer(tokenizer=t, ngram_range=(1, 2), binary=True)),\n",
    "                     ('clf', OneVsRestClassifier(svm.LinearSVC(loss='hinge', fit_intercept=False, C=0.1))),\n",
    "])\n",
    "\n",
    "# train the model\n",
    "text_clf = text_clf.fit(train['data'], train['target'])\n",
    "\n",
    "#predict\n",
    "predicted = text_clf.predict(test['data'])\n",
    "#print type(predicted)\n",
    "#print test['target'].values\n",
    "print np.mean(predicted == test['target'].values)\n",
    "#print metrics.classification_report(test['target'], predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.513449965471\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "OneVsOneClassifier(LinearSVC(random_state=0))\n",
    "\n",
    "text_clf2 = Pipeline([('vect', TfidfVectorizer(tokenizer=t, ngram_range=(1, 2), binary=True)),\n",
    "                      ('clf', OneVsOneClassifier(svm.LinearSVC(random_state=0)))])\n",
    "\n",
    "text_clf2 = text_clf2.fit(train['data'], train['target'])\n",
    "\n",
    "predicted = text_clf2.predict(test['data'])\n",
    "#print type(predicted)\n",
    "#print test['target'].values\n",
    "print np.mean(predicted == test['target'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.541533756454\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "text_clf3 = Pipeline([('vect', TfidfVectorizer(tokenizer = t, ngram_range=(1,2), binary = True)),('clf', LogisticRegression(multi_class = 'ovr'))])\n",
    "\n",
    "text_clf3 = text_clf3.fit(train['data'], train['target'])\n",
    "\n",
    "predicted3 = text_clf3.predict(test['data'])\n",
    "\n",
    "print np.mean(predicted3==test['target'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
